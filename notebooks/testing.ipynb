{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation Notebook  ##\n",
    "\n",
    "This notebook contains codes for various experimation tried for any thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dataset for the training ###\n",
    "\n",
    "* Improve performance to fetch data for training model in GPU, so that GPU does not wait much while data is prepared.\n",
    "* Experiment with various technique with tf.Data.Dataset to check avg time it take to prepare data with a given batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text input to model is a sequence. e.g \"green dress with white top\" will generate data points as \n",
    "<table>\n",
    "<tr><td>index</td><td>X</td><td>Y</td></tr>\n",
    "<tr><td>0</td><td>[start]</td><td>[green]</td></tr>\n",
    "<tr><td>1</td><td>[green]</td><td>[dress]</td></tr>\n",
    "<tr><td>2</td><td>[green dress]</td><td>[with]</td><tr>\n",
    "<tr><td>3</td><td>[green dress with]</td><td>[white]</td><tr>\n",
    "<tr><td>4</td><td>[green dress with white]</td><td>[top]</td><tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial 1 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "def data_generator(dbset, token_obj, max_length, vocab_size, name):\n",
    "  \n",
    "    HDF5_FILEPATH = os.path.join(PROJECT_ROOT, 'data', 'processed', name)\n",
    "    x_name = 'np_image'\n",
    "    y_name = 'id_image'\n",
    "\n",
    "    with h5py.File(HDF5_FILEPATH, 'r') as hdf5_file:\n",
    "\n",
    "        record_cnt = hdf5_file[y_name].shape[0]\n",
    "        map_image_id_index = dict(zip(hdf5_file[y_name][:].flatten(), range(record_cnt)))\n",
    "\n",
    "        #for index in range(dbset.shape[0]): #Error, as loc retrive records based on the index value. If index value not present it will raise error\n",
    "        for index in dbset.index:\n",
    "            \n",
    "            image_id = int(dbset.loc[index,'id'])\n",
    "            caption = str(dbset.loc[index,'preprocess_caption'])\n",
    "\n",
    "            image_id_index = int(map_image_id_index[image_id])\n",
    "            np_image = hdf5_file[x_name][image_id_index]\n",
    "\n",
    "            #input_sequence, output_word = create_sequences_dataset(dbset.loc[index,:], token_obj, max_length, vocab_size)\n",
    "            np_image, input_sequence, output_word = create_sequences_dataset(caption, token_obj, max_length, vocab_size, np_image)           \n",
    "\n",
    "            yield [[np_image, input_sequence], output_word]\n",
    "\n",
    "            del [np_image, input_sequence, output_word]    \n",
    "\n",
    "#def create_sequences_dataset(dbset, token_obj, max_length, vocab_size):\n",
    "def create_sequences_dataset(desc, token_obj, max_length, vocab_size, np_image):\n",
    "\n",
    "    #image_id, X, y = np.array(), np.array(), np.array()\n",
    "    image_ids , X, y = None, None, None\n",
    "    \n",
    "    #text_token_seq = token_obj.texts_to_sequences(list(dbset['preprocess_caption'])) #Output seq of each caption text in the dataset \n",
    "    text_token_seq = token_obj.texts_to_sequences([desc])\n",
    "    #np_image = np.expand_dims(np_image, axis = 0)\n",
    "\n",
    "    for seq in text_token_seq:\n",
    "      \n",
    "        # split one sequence into multiple X,y pairs\n",
    "        image_seq, in_seq, out_seq = list(), list(), list() \n",
    "\n",
    "        for i in range(1, len(seq)):\n",
    "\n",
    "            # split into input and output pair\n",
    "            in_seq.append(seq[:i])\n",
    "            out_seq.append(seq[i]) \n",
    "            image_seq.append(np_image)\n",
    "            \n",
    "        # pad input sequence\n",
    "        in_seq = pad_sequences(in_seq, maxlen = max_length)\n",
    "        \n",
    "        # encode output sequence\n",
    "        out_seq = to_categorical(out_seq, num_classes = vocab_size)\n",
    "        \n",
    "        # store\n",
    "        if type(X) == type(None):\n",
    "\n",
    "            #image_id = np.array([dbset.iloc[index, 0]] * len(in_seq)) #Column location of 'id': 0\n",
    "            image_ids = np.array(image_seq)\n",
    "            X = np.array(in_seq)\n",
    "            y = np.array(out_seq)\n",
    "            \n",
    "          \n",
    "        else:\n",
    "\n",
    "            #image_id = np.append(image_id, np.array([dbset.iloc[index, 0]] * len(in_seq)), axis = 0) #Column location of 'id': 0\n",
    "            image_ids = np.append(image_ids, np.array(image_seq), axis = 0)\n",
    "            X = np.append(X, np.array(in_seq), axis = 0)\n",
    "            y = np.append(y, np.array(out_seq), axis = 0)\n",
    "\n",
    "        del [image_seq, in_seq, out_seq]\n",
    "\n",
    "    return image_ids, X, y\n",
    "    #return np.array(image_id), np.array(X), np.array(y)\n",
    "\n",
    "#Sample code to check\n",
    "#[a,b],c = next(data_generator(db_train, token_obj, max_length, vocab_size, 'train_data.h5'))\n",
    "#a.shape, b.shape, c.shape \n",
    "#>>>((7, 1024, 1024, 3), (7, 15), (7, 10613))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: \n",
    "* Above generator technique where we create in_seq and out_seq during training time, takes lot of time, \n",
    "* Since most of the time was spend by CPU to generate the in_seq and out_seq for each datapoint. \n",
    "* So tried alternative approach to precompute in_seq, out_seq, image_idx. Then saved this precomputed to a file.\n",
    "* So at time of training it only has to read the hdf5 to get numpy represention of image using the hdf5 index for each image.\n",
    "* Average time to read numpy image represenation from hdf5 was 1.2 sec(Including opening the file and read the image by there index position).\n",
    "\n",
    "According to tensorflow doc: tf with generator, While this is a convenient approach it has limited portability and scalability. It must run in the same python process that created the generator, and is still subject to the Python GIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial 2 ####\n",
    "\n",
    "In this try we will discuss problem faced when generating precomputed in_seq, out_seq and image_id files based on the understanding from trail 1 comment.\n",
    "\n",
    "* Problem faced was the even if the pre-computed file was of size 3.5MB. Faced multiple \"memory\" error when try to save and load the file. In case of GPU system in kaggel notebook it always gave error. When generated in_out_seq file on laptop it ran sucessfully without memory issue(had to close all the application and keep RAM empty by removing unwanted process). The memory issue was because of out_seq contain list type with size of each list is 10613."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save in_out seq data #####\n",
    "Things tried to save in_out seq file of training data.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try-1: Failed most of the time. But was sucessfully only once and that's how i created the file on my machine.\n",
    "\n",
    "#Faced error \"ArrowMemoryError: realloc of size 8468299776 failed\". \n",
    "train_data.to_parquet(os.path.join(PROJECT_ROOT, 'data', 'processed', 'train_in_seq_data.parquet'), engine = 'pyarrow', compression = 'gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try-2: Failed to save in json format. Save error memory error.\n",
    "\n",
    "train_data.to_json(os.path.join(PROJECT_ROOT, 'data', 'processed', 'train_in_seq_data.parquet')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try-3: Did not tried this code. But noted this just for future reference. \n",
    "# Saving the file in batchs, each batch we updating existing file by appending the new batch in them.\n",
    "# Reference: https://towardsdatascience.com/4-ways-to-write-data-to-parquet-with-python-a-comparison-3c4f54ee5fec\n",
    "\n",
    "import fastparquet as fp\n",
    "\n",
    "# SETTING BATCH SIZE\n",
    "batch_size = 128\n",
    "\n",
    "# SEQUENTIALLY APPEND TO PARQUET FILE\n",
    "file_name: os.path.join(PROJECT_ROOT, 'data', 'processed', 'train_in_seq_data.parquet')\n",
    "start_idx = 0\n",
    "\n",
    "for end_idx in range(128, range(train_data.shape[0])):\n",
    "\n",
    "    # Write the rows to the Parquet file\n",
    "    fp.write(file_name, train_data[start_idx: end_idx], append = True, compression = 'GZIP')\n",
    "\n",
    "    start_idx = end_idx\n",
    "\n",
    "print(f'Full parquet file named \"%s\" has been written to disk with %s total rows', os.path.join('data', 'processed', 'train_in_seq_data.parquet'), train_data.shape[0])\n",
    "\n",
    "del [train_data]\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try-4: Tried saving the file in chunk using the property in the to_parquet. It uses \"row_group_offsets\" propery when saving to parquet file and require to engine='fastparquet' at time of saving. \n",
    "\n",
    "#Refer: https://www.practicaldatascience.org/html/parquet.html#:~:text=Parquet%20allows%20chunking%2C%20but%20not,you%20can%20chunk%20a%20csv\n",
    "\n",
    "# Also gave error \"The truth value of an array with more than one element is ambiguous. Use a.any() or a.all() parquat\"\n",
    "#This error could be due to fact that we are using list of list.\n",
    "\n",
    "#Since file got created once sucessfully using the code at mention in \"Try-1\" did not search more on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load in_out seq data #####\n",
    "\n",
    "Similary to memory issue faced when saving, faced memory issue when loading the file. Mainly in kaggel GPU system which had 13 GB RAM available. \n",
    "Did not face any issue when working with CPU with 30 GB RAM.\n",
    "\n",
    "* Advantage of reading the parquat file in chunk is, do not need to load then then entire dataset in memory. We shall load as in demand. \n",
    "* Since we use \"pyarrow\" engine when saving the file, it has a option to read the records in batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving with pyarrow engine, can use read a file using pq.ParquetFile and iterate over with a batch size\n",
    "#https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetFile.html#pyarrow.parquet.ParquetFile.iter_batches\n",
    "\n",
    "# Sample code\n",
    "%%time\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "DATASET_FILEPATH = os.path.join(PROJECT_ROOT, 'train_in_seq_data.parquet')\n",
    "parquet_obj = pq.ParquetFile(DATASET_FILEPATH)\n",
    "cnt = 0\n",
    "for dbset in parquet_obj.iter_batches(batch_size = 100):\n",
    "    dbset = dbset.to_pandas()\n",
    "    cnt += dbset.shape[0]\n",
    "    print(dbset.head())\n",
    "print(cnt)\n",
    "\n",
    "#Will use this approach for data_generation for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial 3 ####\n",
    "\n",
    "Case when entire train in_out seq file can be loaded in the memory. In that case we can generate \"map\" in tf.data.Dataset to get data for trail. \n",
    "\n",
    "* Using \"map\" to generate dataset. Tried this approch since with map we can parallelize the task\n",
    "* Refer: https://medium.com/@acordier/tf-data-dataset-generators-with-parallelization-the-easy-way-b5c5f7d2a18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def your_processing_function(i, name):\n",
    "    \n",
    "\n",
    "    if 'valid' in name:\n",
    "        dbset = valid_data\n",
    "    else:\n",
    "        dbset = train_data\n",
    "       \n",
    "\n",
    "    HDF5_FILEPATH = os.path.join(PROJECT_ROOT, 'data', 'processed', name)\n",
    "    x_name = 'np_image'\n",
    "    \n",
    "    with h5py.File(HDF5_FILEPATH, 'r') as hdf5_file: \n",
    "        \n",
    "        image_id_idx = dbset.loc[i, 'image_id_idx']\n",
    "        image_np = hdf5_file[x_name][image_id_idx]\n",
    "        \n",
    "        in_seq = dbset.loc[i, 'in_seq']\n",
    "        \n",
    "        out_seq = dbset.loc[i, 'out_seq']\n",
    "        \n",
    "        return image_np, in_seq , out_seq\n",
    "\n",
    "def func(i, name):\n",
    "    \n",
    "    i = i.numpy() # Decoding from the EagerTensor object\n",
    "    name = str(name.numpy(), 'UTF-8')\n",
    "        \n",
    "    img, in_seq, out_seq = your_processing_function(i, name)\n",
    "    return img, in_seq, out_seq\n",
    "\n",
    "z = list(range(len(train_data))) # The index generator\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(lambda: z, tf.uint32)\n",
    "#With tf.py_function we can write out python code i.e if,else etc. Input arg are passed as tensor\n",
    "dataset = dataset.map(lambda i: tf.py_function(func=func, \n",
    "                                               inp = [i, 'train_data.h5'], \n",
    "                                               Tout= [tf.float16, tf.int32, tf.float16]\n",
    "                                               ), \n",
    "                      num_parallel_calls = tf.data.AUTOTUNE)\n",
    "dataset = dataset.batch(128)\n",
    "\n",
    "#for x,y,z in dataset:\n",
    "#    print(x.shape)\n",
    "#    print(y.shape)\n",
    "#    print(y)\n",
    "#    print(z.shape)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:**\n",
    "* Using \"map\" with tf.data.Dataset it still took 1min 58s to generate batch 128 records "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial 4 #### \n",
    "\n",
    "Reading the train in_out seq file in chunk create a new data_generate that can be used for training the model in GPU system.\n",
    " \n",
    "Advantage: do not need to load the entire train/valid data in memory, will load as an when need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample code, how train data was loaded\n",
    "#With limited RAM in GPU System. Loading the train data fails. So went to alternative approach to read the data in chunk\n",
    "#train_data = pd.read_parquet(os.path.join(PROJECT_ROOT, 'data', 'processed', 'train_in_seq_data.parquet'),\n",
    "#                            columns = ['image_id_idx', 'in_seq', 'out_seq'],\n",
    "#                            engine = 'pyarrow',\n",
    "#)\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import h5py\n",
    "\n",
    "def next_dbset_batch(parquet_obj, size = 10000):\n",
    "    \n",
    "    for dbset in parquet_obj.iter_batches(batch_size = size):\n",
    "        yield dbset\n",
    "\n",
    "def data_generator(name, batch_size): \n",
    "    \n",
    "    #print(name) output: b'train_data.h5'. Here 'b' in output mease byte representation\n",
    "    name = str(name, 'UTF-8') #This  convert bytes to a string\n",
    "    #batch_size = batch_size.numpy()\n",
    "\n",
    "    if 'valid' in name:\n",
    "\n",
    "        #dbset = valid_data\n",
    "        DATASET_FILEPATH = 'valid_in_seq_data.parquet'\n",
    "        HDF5_FILEPATH = 'validate_data.h5'\n",
    "        \n",
    "    else:\n",
    "\n",
    "        #dbset = train_data\n",
    "        DATASET_FILEPATH = 'train_in_seq_data.parquet'\n",
    "        HDF5_FILEPATH = 'train_data.h5'\n",
    "\n",
    "    DATASET_FILEPATH = os.path.join(PROJECT_ROOT, DATASET_FILEPATH) \n",
    "    parquet_obj = pq.ParquetFile(DATASET_FILEPATH)\n",
    "\n",
    "    HDF5_FILEPATH = os.path.join(PROJECT_ROOT, 'data', 'processed', HDF5_FILEPATH)\n",
    "    x_name = 'np_image'\n",
    "\n",
    "    with h5py.File(HDF5_FILEPATH, 'r') as hdf5_file:        \n",
    "        \n",
    "        #records_cnt = hdf5_file[x_name].shape[0]\n",
    "        for dbset in next_dbset_batch(parquet_obj, batch_size): #read the data in chunk\n",
    "\n",
    "            dbset = dbset.to_pandas()\n",
    "            records_cnt = dbset.shape[0]\n",
    "            \n",
    "            for idx in range(records_cnt):\n",
    "\n",
    "                image_id_idx = dbset.loc[idx, 'image_id_idx']\n",
    "                in_image = hdf5_file[x_name][image_id_idx] \n",
    "                in_seq = dbset.loc[idx, 'in_seq']\n",
    "\n",
    "                out_seq = dbset.loc[idx, 'out_seq']\n",
    "                yield ((np.array(in_image), np.array(in_seq.tolist())), np.array(out_seq.tolist())) \n",
    "\n",
    "\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "train_batch = (Dataset\n",
    "              .from_generator(data_generator, \n",
    "                              args = ['train', 25000], #batch_size\n",
    "                              output_signature = (\n",
    "                                                    (\n",
    "                                                      tf.TensorSpec(shape = (1024, 1024, 3), dtype = tf.float16), \n",
    "                                                      tf.TensorSpec(shape = (max_in_seq_len,), dtype = tf.int32)\n",
    "                                                    ),\n",
    "                                                    tf.TensorSpec(shape = (vocab_size,), dtype = tf.float16)\n",
    "                                                  )\n",
    "                            )              \n",
    "              .prefetch(batch_size*2) \n",
    "              .batch(batch_size)                             \n",
    "              )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:**\n",
    "1. This approach did not help in the time taken to fetch the data.\n",
    "\n",
    "\n",
    "Did futher analysis to better understand where most of the time is spent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import h5py\n",
    "import os\n",
    "import time\n",
    "\n",
    "def data_generator(name, batch_size): \n",
    "    \n",
    "    name = str(name, 'UTF-8')\n",
    "    \n",
    "    if 'valid' in name:\n",
    "        \n",
    "        #dbset = valid_data\n",
    "        DATASET_FILEPATH = 'valid_in_seq_data.parquet'    \n",
    "        HDF5_FILEPATH = os.path.join('validate_data', 'validate_data.h5')\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #dbset = train_data\n",
    "        DATASET_FILEPATH = 'train_in_seq_data.parquet'\n",
    "        HDF5_FILEPATH = os.path.join('train_data', 'train_data.h5')\n",
    "\n",
    "    DATASET_FILEPATH = os.path.join(PROJECT_ROOT, DATASET_FILEPATH)\n",
    "    parquet_obj = pq.ParquetFile(DATASET_FILEPATH)    \n",
    "    \n",
    "    HDF5_FILEPATH = os.path.join(PROJECT_ROOT, HDF5_FILEPATH)\n",
    "    x_name = 'np_image'\n",
    "    \n",
    "    #with h5py.File(HDF5_FILEPATH, 'r') as hdf5_file: $uncomment this like to check the time taken when read image numpy instead of image_id \n",
    "    for dbset in next_dbset_batch(parquet_obj, batch_size): #read the data in chunk\n",
    "\n",
    "        dbset = dbset.to_pandas()\n",
    "        records_cnt = dbset.shape[0]\n",
    "        \n",
    "        for idx in range(records_cnt):\n",
    "            \n",
    "            image_id_idx = dbset.loc[idx, 'image_id_idx']\n",
    "            #in_image = hdf5_file[x_name][image_id_idx] \n",
    "                                \n",
    "            in_seq = dbset.loc[idx, 'in_seq']\n",
    "\n",
    "            out_seq = dbset.loc[idx, 'out_seq']\n",
    "            \n",
    "            yield image_id_idx, in_seq.tolist(), out_seq.tolist() \n",
    "            #yield in_image, in_seq.tolist(), out_seq.tolist() #Uncomment to return numpy image instead of image_idx\n",
    "\n",
    "        del [dbset]\n",
    "        gc.collect()\n",
    "    \n",
    "    parquet_obj.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.data import Dataset\n",
    "import tensorflow as tf \n",
    "\n",
    "batch_size = 90\n",
    "max_in_seq_len = 15\n",
    "vocab_size = 10613\n",
    "\n",
    "train_batch = (Dataset\n",
    "              .from_generator(data_generator, \n",
    "                              args = ['train', 25000], #batch_size\n",
    "                              output_signature = (\n",
    "                                                      tf.TensorSpec(shape = (), dtype = tf.int32)\n",
    "                                                      #tf.TensorSpec(shape = (1024,1024,3), dtype = tf.float16), #Uncomment when numpy image is retrun instead of image_idx\n",
    "                                                      tf.TensorSpec(shape = (max_in_seq_len,), dtype = tf.int32),                                                    \n",
    "                                                      tf.TensorSpec(shape = (vocab_size,), dtype = tf.float16)\n",
    "                                                  )\n",
    "                            )              \n",
    "              .prefetch(batch_size*2) \n",
    "              .batch(batch_size)              \n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "idx = 0\n",
    "for x,y,z in train_batch:\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f'Batch: {idx}, elapse time: {(end - start)}')\n",
    "    \n",
    "    #print(x.shape)    \n",
    "    #print(y.shape) \n",
    "    #print(z.shape) \n",
    "    idx +=1\n",
    "    start = time.time()\n",
    "    if idx == 3:    \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:**\n",
    "* It took 5sec to generate 270 records in batch. \n",
    "\n",
    "So most of the time is spend to read numpy array from hdf5 file. When tried the above code after modification to read the numpy image array it took 5m aprox to read 270 records.\n",
    "\n",
    "So now tried various approach to speed up the read time from hdf5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hdf5 file object as a global varaible \n",
    "\n",
    "hdf5_file = h5py.File('/kaggle/input/fashion-image-encoding/train_data/train_data.h5', 'r') #Define hdf5 object at global level\n",
    "PROJECT_ROOT = '/kaggle/input/fashion-image-encoding'\n",
    "def next_dbset_batch(parquet_obj, size = 10000):\n",
    "    \n",
    "    for dbset in parquet_obj.iter_batches(batch_size = size, columns = ['image_id_idx', 'in_seq', 'out_seq']):\n",
    "        yield dbset\n",
    "\n",
    "\n",
    "def data_generator(name, batch_size): \n",
    "    \n",
    "    name = str(name, 'UTF-8')\n",
    "    \n",
    "    if 'valid' in name:\n",
    "        \n",
    "        #dbset = valid_data\n",
    "        DATASET_FILEPATH = 'valid_in_seq_data.parquet'    \n",
    "        HDF5_FILEPATH = os.path.join('validate_data', 'validate_data.h5')\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #dbset = train_data\n",
    "        DATASET_FILEPATH = 'train_in_seq_data.parquet'\n",
    "        HDF5_FILEPATH = os.path.join('train_data', 'train_data.h5')\n",
    "\n",
    "    DATASET_FILEPATH = os.path.join(PROJECT_ROOT, DATASET_FILEPATH)\n",
    "    parquet_obj = pq.ParquetFile(DATASET_FILEPATH)\n",
    "    \n",
    "    HDF5_FILEPATH = os.path.join(PROJECT_ROOT, HDF5_FILEPATH)\n",
    "    x_name = 'np_image'  \n",
    "        \n",
    "\n",
    "    #records_cnt = hdf5_file[x_name].shape[0]\n",
    "    for dbset in next_dbset_batch(parquet_obj, batch_size): #read the data in chunk\n",
    "\n",
    "        dbset = dbset.to_pandas()\n",
    "        records_cnt = dbset.shape[0]\n",
    "        \n",
    "        #since we know that the records will be in sequence. \n",
    "        \n",
    "        for idx in range(records_cnt):\n",
    "\n",
    "            image_id_idx = dbset.loc[idx, 'image_id_idx']\n",
    "            in_image = hdf5_file[x_name][image_id_idx] \n",
    "            \n",
    "            in_seq = dbset.loc[idx, 'in_seq']\n",
    "\n",
    "            out_seq = dbset.loc[idx, 'out_seq']\n",
    "            yield in_image, in_seq.tolist(), out_seq.tolist()\n",
    "        \n",
    "\n",
    "        del [dbset]\n",
    "        gc.collect()\n",
    "    \n",
    "    parquet_obj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:**\n",
    "\n",
    "* When define hdf5 file object global varaible, time taken for 270 records reduce to 4min. It took 0.85 sec approx to read the a single value from HDF5.\n",
    "\n",
    "Let's try if we use \"map\" with parallization if we can improve this time.\n",
    "1. When hdf5 is define global\n",
    "2. When hdf5 is not define global\n",
    "In both the above case we will use \"map\" to parallization to note the time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When hdf5 is define global/not global\n",
    "\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import h5py\n",
    "import os\n",
    "import time\n",
    "\n",
    "#hdf5_file = h5py.File('/kaggle/input/fashion-image-encoding/train_data/train_data.h5', 'r') #Define hdf5 object at global level.Modify the code when want to try with global hdf5 object\n",
    "\n",
    "PROJECT_ROOT = '/kaggle/input/fashion-image-encoding'\n",
    "def next_dbset_batch(parquet_obj, size = 10000):\n",
    "    \n",
    "    for dbset in parquet_obj.iter_batches(batch_size = size, columns = ['image_id_idx', 'in_seq', 'out_seq']):\n",
    "        yield dbset\n",
    "\n",
    "def map_generator(name, image_id_idx, in_seq, out_seq):#*inp\n",
    "    \n",
    "    #image_id_idx = inp[0][0]\n",
    "    #in_seq = inp[0][1]\n",
    "    #out_seq = inp[1]\n",
    "    \n",
    "    #print(image_id_idx.numpy())\n",
    "    #print(image_id_idx)\n",
    "    #print(in_seq)\n",
    "    #print(out_seq)\n",
    "        \n",
    "    name = str(name.numpy(), 'UTF-8')\n",
    "    #print(name)\n",
    "    \n",
    "    if 'valid' in name:\n",
    "        \n",
    "        #dbset = valid_data\n",
    "        #DATASET_FILEPATH = 'valid_in_seq_data.parquet'    \n",
    "        HDF5_FILEPATH = os.path.join('validate_data', 'validate_data.h5')\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #dbset = train_data\n",
    "        #DATASET_FILEPATH = 'train_in_seq_data.parquet'\n",
    "        HDF5_FILEPATH = os.path.join('train_data', 'train_data.h5')\n",
    "    \n",
    "    HDF5_FILEPATH = os.path.join(PROJECT_ROOT, HDF5_FILEPATH)\n",
    "    x_name = 'np_image'\n",
    "    \n",
    "    start = time.time()\n",
    "    with h5py.File(HDF5_FILEPATH, 'r') as hdf5_file: #Comment this line when want to use global hdf5 object\n",
    "        \n",
    "        #print(\"opened hdf5\")\n",
    "        in_image = hdf5_file[x_name][image_id_idx.numpy()] \n",
    "            #print(\"access opened hdf5\")\n",
    "            #print(in_image.shape)\n",
    "        end = time.time()\n",
    "        print(f'Elapse time to get the records: {(end - start)}')\n",
    "\n",
    "        return in_image , in_seq, out_seq\n",
    "    \n",
    "    #return tf.convert_to_tensor(in_image) , in_seq, out_seq\n",
    "        #return (in_image, in_seq), out_seq\n",
    "\n",
    "def data_generator(name, batch_size): \n",
    "    \n",
    "    name = str(name, 'UTF-8')\n",
    "    \n",
    "    if 'valid' in name:\n",
    "        \n",
    "        #dbset = valid_data\n",
    "        DATASET_FILEPATH = 'valid_in_seq_data.parquet'    \n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #dbset = train_data\n",
    "        DATASET_FILEPATH = 'train_in_seq_data.parquet'\n",
    "\n",
    "    DATASET_FILEPATH = os.path.join(PROJECT_ROOT, DATASET_FILEPATH)\n",
    "    parquet_obj = pq.ParquetFile(DATASET_FILEPATH)\n",
    "    \n",
    "    #records_cnt = hdf5_file[x_name].shape[0]\n",
    "    for dbset in next_dbset_batch(parquet_obj, batch_size): #read the data in chunk\n",
    "\n",
    "        dbset = dbset.to_pandas()\n",
    "        records_cnt = dbset.shape[0]\n",
    "                    \n",
    "        for idx in range(records_cnt):\n",
    "\n",
    "            image_id_idx = dbset.loc[idx, 'image_id_idx']\n",
    "                \n",
    "            in_seq = dbset.loc[idx, 'in_seq']\n",
    "\n",
    "            out_seq = dbset.loc[idx, 'out_seq']\n",
    "            yield image_id_idx, in_seq.tolist(), out_seq.tolist() #used with map in tf.data\n",
    "            \n",
    "        del [dbset]\n",
    "        gc.collect()\n",
    "    \n",
    "    parquet_obj.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.data import Dataset\n",
    "import tensorflow as tf \n",
    "\n",
    "batch_size = 90\n",
    "max_in_seq_len = 15\n",
    "vocab_size = 10613\n",
    "\n",
    "train_batch = (Dataset\n",
    "              .from_generator(data_generator, \n",
    "                              args = ['train', 25000], #batch_size\n",
    "                              output_signature = (\n",
    "                                                     \n",
    "                                                      tf.TensorSpec(shape = (), dtype = tf.int32), \n",
    "                                                      tf.TensorSpec(shape = (max_in_seq_len,), dtype = tf.int32),\n",
    "                                                    \n",
    "                                                      tf.TensorSpec(shape = (vocab_size,), dtype = tf.float16)\n",
    "                                                  )\n",
    "                            )              \n",
    "              .prefetch(batch_size*2) \n",
    "              #.map(lambda x,y,z: map_generator('train',x,y,z)) #Code tried to access without tf.py_funnction. # But will not work as image_idx will in in form of tensor. and fail to access the tensor value. Refer later code to understand \n",
    "              .map(lambda im,i,o: tf.py_function(func = map_generator, \n",
    "                                               inp = ['train',im,i,o],\n",
    "                                                 Tout= [\n",
    "                                                       \n",
    "                                                      tf.TensorSpec(shape = (1024,1024,3), dtype = tf.float16), \n",
    "                                                      tf.TensorSpec(shape = (max_in_seq_len,), dtype = tf.int32),\n",
    "                                                       \n",
    "                                                      tf.TensorSpec(shape = (vocab_size,), dtype = tf.float16)\n",
    "                                               ]                                               \n",
    "                                               ) \n",
    "                    ,num_parallel_calls = tf.data.AUTOTUNE\n",
    "                  )\n",
    "              .batch(batch_size)\n",
    "              \n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "idx = 0\n",
    "for x,y,z in train_batch:\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f'Batch: {idx}, elapse time: {(end - start)}')\n",
    "    \n",
    "    #print(x.shape)    \n",
    "    #print(y.shape) \n",
    "    #print(z.shape) \n",
    "    idx +=1\n",
    "    start = time.time()\n",
    "    if idx == 3:    \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:**\n",
    "* Define hdf5 file object with global varaible with 270 records and running map in parallel did not help at all. It took 4min only. No improvement. One think observed that time take to read an individual records from hdf5 increased. Now it take 3.4sec approx to read a single value from hdf5. May be this is the reason it did not perform well in parallel. Reason for increase in time map be due to GIL problem in Python.\n",
    "* When define hdf5 file within the generator with map in parallel. No improvement to retrive time still 4m to access 270 records.\n",
    "\n",
    "\n",
    "##### We can not reduce the read time for hdf5. Only way is to reduce the number of read from the hdf5 #####\n",
    "As the input data will be in sequence. So when we read a nupy image for a datapoint. We can use the same numpy image array for next few datapoints which are part of the previous datapoint.\n",
    "\n",
    "When tried this approach it took 39.5sec to read 270 records. Which very fast.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/burhanuddinlatsaheb/image-captioning-vit-gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a8019b6faff98163eb884b931cd42fce4cc159f4b6cbdadf8184d77ab18833d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
