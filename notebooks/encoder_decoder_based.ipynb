{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Caption using Encoder and Decoder (CNN-RNN) # \n",
    "\n",
    "In this notebook we use merge architectures encoder-decoder recurrent neural network models on caption generation.\n",
    "This involves two elements:\n",
    "\n",
    "1. Encoder: pre-trained convolutional neural network model that reads the image input and encodes the content into a fixed-length vector using an internal representation. Output of the encoder is an hidden unit/context generated by reading the input which will be passed to all the decoder.\n",
    "2. Decoder: model that reads the encoded image and generates the textual description output.\n",
    "\n",
    "In merge model architecture,  combines both the encoded form of the image input with the encoded form of the text description generated. The combination of these two encoded inputs is then used by a very simple decoder model to generate the next word in the sequence. The approach uses the recurrent neural network only to encode the text generated so far.\n",
    "\n",
    "![Merge Architecture for Encoder-Decoder Model](..\\images\\merge_model.png)\n",
    "\n",
    "Reference: https://machinelearningmastery.com/caption-generation-inject-merge-architectures-encoder-decoder-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import src.utils as plp\n",
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "import os\n",
    "import gc\n",
    "\n",
    "PROJECT_ROOT = plp.get_project_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training  ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pyarrow.parquet as pq\n",
    "import h5py\n",
    "\n",
    "def next_dbset_batch(parquet_obj, size = 10000):\n",
    "    \n",
    "    for dbset in parquet_obj.iter_batches(batch_size = size, columns = ['image_id_idx', 'in_seq', 'out_seq']):\n",
    "        yield dbset\n",
    "\n",
    "def data_generator(name, batch_size): \n",
    "    \n",
    "    #print(name) output: b'train_data.h5'. Here 'b' in output mease byte representation\n",
    "    name = str(name, 'UTF-8') #This  convert bytes to a string\n",
    "    #batch_size = batch_size.numpy()\n",
    "\n",
    "    if 'valid' in name:\n",
    "\n",
    "        #dbset = valid_data\n",
    "        DATASET_FILEPATH = 'valid_in_seq_data.parquet'\n",
    "        HDF5_FILEPATH = 'validate_data.h5'\n",
    "        \n",
    "    else:\n",
    "\n",
    "        #dbset = train_data\n",
    "        DATASET_FILEPATH = 'train_in_seq_data.parquet'\n",
    "        HDF5_FILEPATH = 'train_data.h5'\n",
    "\n",
    "    DATASET_FILEPATH = os.path.join(PROJECT_ROOT, 'data', 'processed', DATASET_FILEPATH) \n",
    "    parquet_obj = pq.ParquetFile(DATASET_FILEPATH)\n",
    "\n",
    "    HDF5_FILEPATH = os.path.join(PROJECT_ROOT, 'data', 'processed', HDF5_FILEPATH)\n",
    "    x_name = 'np_image'\n",
    "\n",
    "    with h5py.File(HDF5_FILEPATH, 'r') as hdf5_file:        \n",
    "        \n",
    "        #records_cnt = hdf5_file[x_name].shape[0]\n",
    "        for dbset in next_dbset_batch(parquet_obj, batch_size): #read the data in chunk\n",
    "\n",
    "            dbset = dbset.to_pandas()\n",
    "            records_cnt = dbset.shape[0]\n",
    "            \n",
    "            #since we know that the records will be in sequence of same input text\n",
    "            prev_image_id_idx = -1\n",
    "            for idx in range(records_cnt):\n",
    "\n",
    "                image_id_idx = dbset.loc[idx, 'image_id_idx']\n",
    "                #If previous image_id and current image_idx not same load the new in_image numpy array. Else use the previous numpy array    \n",
    "                #This will reduce IO operation time required to read the image data again and help to retrive data for train faster\n",
    "                if prev_image_id_idx != image_id_idx: \n",
    "                    in_image = hdf5_file[x_name][image_id_idx] \n",
    "\n",
    "                in_seq = dbset.loc[idx, 'in_seq']\n",
    "\n",
    "                out_seq = dbset.loc[idx, 'out_seq']\n",
    "\n",
    "                yield ((in_image, in_seq.tolist()), out_seq.tolist()) \n",
    "                #yield ((np.array(in_image), np.array(in_seq.tolist())), np.array(out_seq.tolist())) \n",
    "\n",
    "                prev_image_id_idx = image_id_idx\n",
    "            \n",
    "            del [dbset, in_image, in_seq, out_seq]\n",
    "            gc.collect()\n",
    "\n",
    "    parquet_obj.close()\n",
    "\n",
    "\"\"\"\n",
    "#Code to verify the code\n",
    "%%time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "start = time.time()\n",
    "idx = 0\n",
    "for (x,y),z in train_batch:\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f'Batch: {idx}, elapse time: {(end - start)}')\n",
    "    \n",
    "    i = 0\n",
    "    for image in x.numpy():\n",
    "        image = (image*255).astype(int)\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        i+=1\n",
    "        if i == 2 :\n",
    "            break\n",
    "    break\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect TPU,multiple GPU, return appropriate distribution strategy\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "is_TPU_instance_Init = False\n",
    "is_Multiple_GPU_instance_Init = False\n",
    "\n",
    "num_replicas_in_sync = 1\n",
    "\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n",
    "    print('Running on TPU ', tpu.master())\n",
    "    is_TPU_instance_Init = True\n",
    "    \n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    \n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    \n",
    "    num_replicas_in_sync = strategy.num_replicas_in_sync\n",
    "    print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
    "    \n",
    "else: #Check for multiple GPU\n",
    "    \n",
    "    #Setting for multipl GPU https://towardsdatascience.com/train-a-neural-network-on-multi-gpu-with-tensorflow-42fa5f51b8af\n",
    "    #to see the list of available GPU devices doing the following\n",
    "    devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    num_replicas_in_sync = len(devices)\n",
    "    \n",
    "    if num_replicas_in_sync > 1:\n",
    "        is_Multiple_GPU_instance_Init = True\n",
    "        \n",
    "    #Detect multiple GPU then distribute the task on multiple machine\n",
    "    #strategy = tf.distribute.MirroredStrategy() #To Suppress the warning duing run https://github.com/tensorflow/tensorflow/issues/42146\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "    options = tf.data.Options()    \n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "\n",
    "if ((is_Multiple_GPU_instance_Init == False) & (is_TPU_instance_Init == False)):\n",
    "    strategy = tf.distribute.get_strategy() \n",
    "    num_replicas_in_sync = 1\n",
    "    print('General strategy...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 10613, 129214)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since we shall generate data from the data_generatore,so did not open file.\n",
    "#  Let's assign the values that we have learning during the pre-processing stage.\n",
    "\n",
    "#max_in_seq_len = len(train_data.iloc[0, 1])\n",
    "#vocab_size = len(train_data.iloc[0, 2])\n",
    "#record_cnt = train_data.shape[0]\n",
    "\n",
    "max_in_seq_len = 15\n",
    "vocab_size = 10613\n",
    "record_cnt = 129214 #Number of records in training dataset\n",
    "\n",
    "(max_in_seq_len, vocab_size, record_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "with strategy.scope(): \n",
    "    adam_optimizers  = Adam(learning_rate = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import CategoricalCrossentropy\n",
    "\n",
    "with strategy.scope(): \n",
    "    entropy_loss = CategoricalCrossentropy(from_logits = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.xception import Xception\n",
    "from keras.models import load_model\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Add\n",
    "\n",
    "with strategy.scope(): \n",
    "\n",
    "    # define the captioning model\n",
    "    def define_model(vocab_size, max_length):\n",
    "        \n",
    "        # features from the CNN model squeezed from 2048 to 256 nodes    \n",
    "\n",
    "        input_image = Input(shape = (1024, 1024, 3), name = 'np_image')\n",
    "\n",
    "        xception_model = Xception( weights = 'imagenet', \n",
    "                                   include_top = False, \n",
    "                                   pooling = 'avg' , \n",
    "                                   input_tensor = Input(shape = (1024, 1024, 3))\n",
    "                                )\n",
    "        xception_model.trainable = False #Freeze the feature extraction layers\n",
    "        image_extract = xception_model(input_image)\n",
    "\n",
    "        fe1 = Dropout(0.5)(image_extract)\n",
    "        fe2 = Dense(256, activation = 'relu')(fe1)\n",
    "\n",
    "        # LSTM sequence model\n",
    "        inputs_caption = Input(shape = (max_length,), name = 'word_seq')\n",
    "        se1 = Embedding(vocab_size, 256, mask_zero = True)(inputs_caption)\n",
    "        se2 = Dropout(0.5)(se1)\n",
    "        se3 = LSTM(256)(se2)\n",
    "\n",
    "        # Merging both models\n",
    "        decoder1 = Add()([fe2, se3])\n",
    "        decoder2 = Dense(256, activation = 'relu')(decoder1)\n",
    "        outputs = Dense(vocab_size, activation = 'softmax', name = 'output_seq')(decoder2)\n",
    "        \n",
    "        # tie it together [image, seq] [word]\n",
    "        model = Model(inputs = [input_image, inputs_caption], outputs = outputs)\n",
    "        #model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "        \n",
    "        # summarize model\n",
    "        #print(model.summary())\n",
    "        #plot_model(model, to_file ='model.png', show_shapes=True)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    model_path = os.path.join(PROJECT_ROOT, 'data', 'processed', 'my_model.h5' ) \n",
    "\n",
    "    if os.path.isfile(model_path):\n",
    "        print('Loading previous saved model...')\n",
    "        model = load_model(model_path)\n",
    "    else:\n",
    "        model = define_model(vocab_size, max_in_seq_len)\n",
    "        \n",
    "    model.compile(loss = entropy_loss, optimizer = adam_optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())\n",
    "#plot_model(model, show_shapes = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32 #64\n",
    "batch_size = batch_size * num_replicas_in_sync\n",
    "\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refered to decide the order: https://cs230.stanford.edu/blog/datapipeline/#best-practices\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "train_batch = (Dataset\n",
    "              .from_generator(data_generator, \n",
    "                              args = ['train', 25000], \n",
    "                              output_signature = (\n",
    "                                                    (\n",
    "                                                      tf.TensorSpec(shape = (1024, 1024, 3), dtype = tf.float16), \n",
    "                                                      tf.TensorSpec(shape = (max_in_seq_len, ), dtype = tf.int32)\n",
    "                                                    ),\n",
    "                                                    tf.TensorSpec(shape = (vocab_size, ), dtype = tf.float16)\n",
    "                                                 )\n",
    "                            )  \n",
    "              .batch(batch_size)            \n",
    "              .prefetch(tf.data.AUTOTUNE)                                            \n",
    "              )\n",
    "\n",
    "#train_batch = train_batch.prefetch(batch_size).batch(batch_size)  \n",
    "  \n",
    "############-----------------------------------------------------------------------------------------######################\n",
    "#train_generator = data_generator(db_train, token_obj, max_length, vocab_size, 'train_data.h5') #Was too slow\n",
    "\n",
    "#Issue using from_tensor where we define the input data like bellow when access tensor image_idx, was not able to access the value\n",
    "#from tensorflow import constant, uint16, int32, Variable\n",
    "#out_seq_tensor = constant(np.array(train_data['out_seq'].values.tolist()), shape = (record_cnt, vocab_size), dtype = uint16)\n",
    "#in_seq_tensor = constant(np.array(train_data['in_seq'].values.tolist()), shape = (record_cnt, max_in_seq_len), dtype = int32)\n",
    "#image_idx_tensor = constant(train_data['image_id_idx'], shape = (record_cnt,), dtype = int32)\n",
    "#or\n",
    "#image_idx_tensor = train_data['image_id_idx']\n",
    "#There was problem with from_tensor approach:\n",
    "# 1. We need to access the image_id_id in \"map\" to get the numpy represenation of the image by reading hdf5 file.\n",
    "#   In this problem was in map pass value of tensor. To get the value from tensor tried various way like as_numpy, eval, get_static_value etc.\n",
    "#   But due to eagey execution it allway retrun error as \"Tensor does not have attribute \"map\".\n",
    "# 2. Reading the values of the dataframe column and then converting tolist() in high resource consumption. In laptop it always fail.\n",
    "# 3. Each time it call \"map\" we need to open the hdf5 file and read the value and close. This add to the computation. \n",
    "# Advantage of using \"map\" is parallelization\n",
    "\n",
    "#Sample to test \n",
    "#train_batch = train_batch.batch(128)\n",
    "#for (x,y),z in train_batch:\n",
    "    \n",
    "#    print(x.shape)\n",
    "#    print(y.shape)\n",
    "#    print(y)\n",
    "#    print(z.shape)\n",
    "#    break\n",
    "#Above same code took 1min 50sec to generate batch128 records . This is without prefetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_batch = (Dataset\n",
    "              .from_generator(data_generator, \n",
    "                              args = ['valid', 25000], \n",
    "                              output_signature = (\n",
    "                                                    (\n",
    "                                                      tf.TensorSpec(shape = (1024, 1024, 3), dtype = tf.float16), \n",
    "                                                      tf.TensorSpec(shape = (max_in_seq_len,), dtype = tf.int32)\n",
    "                                                    ),\n",
    "                                                    tf.TensorSpec(shape = (vocab_size, ), dtype = tf.float16)\n",
    "                                                 )\n",
    "                            )\n",
    "              .batch(batch_size)            \n",
    "              .prefetch(tf.data.AUTOTUNE)\n",
    "              )\n",
    "\n",
    "#valid_batch = valid_batch.prefetch(batch_size).batch(batch_size) \n",
    "           \n",
    "#valid_generator = data_generator(db_valid, token_obj, max_length, vocab_size, 'validate_data.h5') #Was too slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model\n",
    "history = model.fit(train_batch,\n",
    "                    #batch_size = GLOBAL_BATCH_SIZE\n",
    "                    steps_per_epoch = record_cnt/batch_size,\n",
    "                    epochs = 5,\n",
    "                    shuffle = False,\n",
    "                    #initial_epoch = 1, # We ran one epoch previously\n",
    "                    verbose = 1,\n",
    "                   )\n",
    "                   \n",
    "\"\"\"\n",
    "history = model.fit(x = train_generator,\n",
    "                    steps_per_epoch = db_train.shape[0],\n",
    "                    \n",
    "                    validation_data = valid_generator,\n",
    "                    validation_steps = db_valid.shape[0],    \n",
    "                    validation_freq = 2, \n",
    "                    \n",
    "                    #batch_size = , #As per doc Do not specify the batch_size if your data is in the form of  generators\n",
    "                    epochs = 10,                       \n",
    "                    verbose = 1,\n",
    "\n",
    "                    max_queue_size = 50,\n",
    "                    workers = 10,\n",
    "                    use_multiprocessing = True,\n",
    "                  )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2ad14890164d28bd1000629ce5fb9aeaf6f566b8a3110e77baec0a3539822cf2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
