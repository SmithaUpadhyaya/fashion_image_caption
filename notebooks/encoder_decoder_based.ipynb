{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Image Caption using Encoder and Decoder (CNN-RNN) # \n","\n","In this notebook we use merge architectures encoder-decoder recurrent neural network models on caption generation.\n","This involves two elements:\n","\n","1. Encoder: pre-trained convolutional neural network model that reads the image input and encodes the content into a fixed-length vector using an internal representation. Output of the encoder is an hidden unit/context generated by reading the input which will be passed to all the decoder.\n","2. Decoder: model that reads the encoded image and generates the textual description output.\n","\n","In merge model architecture,  combines both the encoded form of the image input with the encoded form of the text description generated. The combination of these two encoded inputs is then used by a very simple decoder model to generate the next word in the sequence. The approach uses the recurrent neural network only to encode the text generated so far.\n","\n","![Merge Architecture for Encoder-Decoder Model](..\\images\\merge_model.png)\n","\n","Reference: https://machinelearningmastery.com/caption-generation-inject-merge-architectures-encoder-decoder-model/"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-05-04T03:20:52.210709Z","iopub.status.busy":"2023-05-04T03:20:52.210101Z","iopub.status.idle":"2023-05-04T03:20:52.239733Z","shell.execute_reply":"2023-05-04T03:20:52.238761Z","shell.execute_reply.started":"2023-05-04T03:20:52.210661Z"},"trusted":true},"outputs":[],"source":["import src.utils as plp\n","#import pandas as pd\n","#import numpy as np\n","import os\n","import gc\n","\n","PROJECT_ROOT = plp.get_project_root()\n","\n","PROJECT_IMAGE_ROOT = os.path.join(PROJECT_ROOT, 'data', 'processed', 'image_feature_extracted_using_inception') "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Detect TPU,multiple GPU, return appropriate distribution strategy\n","\n","import tensorflow as tf\n","\n","is_TPU_instance_Init = False\n","is_Multiple_GPU_instance_Init = False\n","\n","num_replicas_in_sync = 1\n","\n","try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n","    print('Running on TPU ', tpu.master())\n","    is_TPU_instance_Init = True\n","    \n","except ValueError:\n","    tpu = None\n","\n","if tpu:\n","    \n","    tf.config.experimental_connect_to_cluster(tpu)\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","    \n","    num_replicas_in_sync = strategy.num_replicas_in_sync\n","    print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n","    \n","else: #Check for multiple GPU\n","    \n","    #Setting for multipl GPU https://towardsdatascience.com/train-a-neural-network-on-multi-gpu-with-tensorflow-42fa5f51b8af\n","    #to see the list of available GPU devices doing the following\n","    devices = tf.config.experimental.list_physical_devices('GPU')\n","    num_replicas_in_sync = len(devices)\n","    \n","    if num_replicas_in_sync > 1:\n","        is_Multiple_GPU_instance_Init = True\n","        \n","    #Detect multiple GPU then distribute the task on multiple machine\n","    strategy = tf.distribute.MirroredStrategy() #To Supress the warning duing run https://github.com/tensorflow/tensorflow/issues/42146\n","    #strategy = tf.distribute.MultiWorkerMirroredStrategy()\n","    options = tf.data.Options()    \n","    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n","\n","if ((is_Multiple_GPU_instance_Init == False) & (is_TPU_instance_Init == False)):\n","    strategy = tf.distribute.get_strategy() \n","    num_replicas_in_sync = 1\n","    print('General strategy...')"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-04T03:21:02.953538Z","iopub.status.busy":"2023-05-04T03:21:02.952915Z","iopub.status.idle":"2023-05-04T03:21:02.968492Z","shell.execute_reply":"2023-05-04T03:21:02.966815Z","shell.execute_reply.started":"2023-05-04T03:21:02.953496Z"},"trusted":true},"outputs":[],"source":["import pyarrow.parquet as pq\n","import numpy as np\n","import gc\n","\n","def next_dbset_batch(parquet_obj, size = 10000):\n","    \n","    for dbset in parquet_obj.iter_batches(batch_size = size, columns = ['image_id', 'in_seq', 'out_seq']):\n","        yield dbset\n","\n","#This perfrom worst took 1min 11s for batch size of 90.\n","def data_generator(name, batch_size): \n","    \n","    #print(name) output: b'train_data.h5'. Here 'b' in output mease byte representation\n","    name = str(name, 'UTF-8') #This  convert bytes to a string\n","    \n","    if 'valid' in name:\n","        \n","        #dbset = valid_data\n","        DATASET_FILEPATH = 'valid_in_seq_data.parquet'    \n","        \n","    else:\n","        \n","        #dbset = train_data\n","        DATASET_FILEPATH = 'train_in_seq_data.parquet'\n","    \n","    \n","    DATASET_FILEPATH = os.path.join(PROJECT_ROOT, 'data', 'processed', DATASET_FILEPATH) \n","    parquet_obj = None\n","    \n","    #while not 'valid' in name:\n","    while True:\n","        \n","        if parquet_obj == None:\n","            parquet_obj = pq.ParquetFile(DATASET_FILEPATH)    \n","\n","        #records_cnt = hdf5_file[x_name].shape[0]\n","        for dbset in next_dbset_batch(parquet_obj, batch_size): #read the data in chunk            \n","\n","            dbset = dbset.to_pandas()\n","            records_cnt = dbset.shape[0]\n","\n","            #since we know that the records will be in sequence of same input text\n","            prev_image_id = -1\n","            for idx in range(records_cnt):\n","\n","                image_id = dbset.loc[idx, 'image_id']\n","\n","                if prev_image_id != image_id: #If previous image_id and current image_idx not same load the new in_image numpy array. Else use the previous numpy array    \n","\n","                    img_path = os.path.join(PROJECT_IMAGE_ROOT, str(image_id) + '.npy')      \n","                    in_image = np.load(img_path)\n","\n","                in_seq = dbset.loc[idx, 'in_seq']\n","                out_seq = dbset.loc[idx, 'out_seq']\n","\n","                yield ((in_image, in_seq.tolist()), out_seq.tolist()) \n","\n","                prev_image_id = image_id\n","\n","            del [dbset, in_image, in_seq, out_seq]\n","            gc.collect()\n","\n","        parquet_obj = None\n","                    "]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-05-04T03:21:02.971645Z","iopub.status.busy":"2023-05-04T03:21:02.971202Z","iopub.status.idle":"2023-05-04T03:21:02.986733Z","shell.execute_reply":"2023-05-04T03:21:02.985528Z","shell.execute_reply.started":"2023-05-04T03:21:02.971604Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(15, 10613, 129214)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["max_in_seq_len = 15\n","vocab_size = 10613\n","train_record_cnt = 129214\n","valid_record_cnt = 13747\n","\n","(max_in_seq_len, vocab_size, train_record_cnt)"]},{"cell_type":"markdown","metadata":{},"source":["## HyperParamater Tuning ##"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-05-04T03:21:02.989913Z","iopub.status.busy":"2023-05-04T03:21:02.988317Z","iopub.status.idle":"2023-05-04T03:21:03.002261Z","shell.execute_reply":"2023-05-04T03:21:03.001012Z","shell.execute_reply.started":"2023-05-04T03:21:02.989799Z"},"trusted":true},"outputs":[{"data":{"text/plain":["4096"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["batch_size = 2048 #1024\n","batch_size = batch_size * num_replicas_in_sync\n","\n","batch_size"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-05-04T03:21:03.004756Z","iopub.status.busy":"2023-05-04T03:21:03.004039Z","iopub.status.idle":"2023-05-04T03:21:03.066983Z","shell.execute_reply":"2023-05-04T03:21:03.065918Z","shell.execute_reply.started":"2023-05-04T03:21:03.004715Z"},"trusted":true},"outputs":[],"source":["from tensorflow.data import Dataset\n","\n","train_batch = (Dataset\n","              .from_generator(data_generator, \n","                              args = ['train', 25000], #batch_size\n","                              output_signature = (\n","                                                    (\n","                                                      tf.TensorSpec(shape = (2048, ), dtype = tf.float16), \n","                                                      tf.TensorSpec(shape = (max_in_seq_len,), dtype = tf.int32)\n","                                                    ),\n","                                                    tf.TensorSpec(shape = (vocab_size,), dtype = tf.float16)\n","                                                  )\n","                            )  \n","              .batch(batch_size)   \n","              .prefetch(tf.data.AUTOTUNE)   \n","              )\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-05-04T03:21:03.070693Z","iopub.status.busy":"2023-05-04T03:21:03.069534Z","iopub.status.idle":"2023-05-04T03:21:03.104519Z","shell.execute_reply":"2023-05-04T03:21:03.103537Z","shell.execute_reply.started":"2023-05-04T03:21:03.070649Z"},"trusted":true},"outputs":[],"source":["valid_batch = (Dataset\n","              .from_generator(data_generator, \n","                              args = ['valid', 25000], #batch_size\n","                              output_signature = (\n","                                                    (\n","                                                      tf.TensorSpec(shape = (2048, ), dtype = tf.float16), \n","                                                      tf.TensorSpec(shape = (max_in_seq_len,), dtype = tf.int32)\n","                                                    ),\n","                                                    tf.TensorSpec(shape = (vocab_size, ), dtype = tf.float16)\n","                                                 )\n","                            )\n","              .batch(batch_size) \n","              .prefetch(tf.data.AUTOTUNE)\n","              )              \n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T05:10:05.378543Z","iopub.status.busy":"2023-05-02T05:10:05.378159Z","iopub.status.idle":"2023-05-02T05:10:05.384177Z","shell.execute_reply":"2023-05-02T05:10:05.382941Z","shell.execute_reply.started":"2023-05-02T05:10:05.378505Z"},"trusted":true},"outputs":[],"source":["#Use keras for Distributed Tuning\n","\n","#Use \"export\" to set environment varaible in bash\n","#export KERASTUNER_TUNER_ID = \"chief\"\n","#export KERASTUNER_ORACLE_IP = \"127.0.0.1\"\n","#export KERASTUNER_ORACLE_PORT = \"8000\"\n","\n","#Use \"%env\" to set enviorment varaibe in jupiter notebook\n","\n","#%env KERASTUNER_TUNER_ID = \"chief\"\n","#%env KERASTUNER_ORACLE_IP = \"127.0.0.1\"\n","#%env KERASTUNER_ORACLE_PORT = \"8000\"\n","## %env GRPC_VERBOSITY = 'debug'  #environment variable to see detailed error message. #Did not help with the error \"RuntimeError: Failed to bind to address \"127.0.0.1\":\"8000\"; set GRPC_VERBOSITY=debug environment variable to see detailed error message.\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T05:10:05.387079Z","iopub.status.busy":"2023-05-02T05:10:05.386280Z","iopub.status.idle":"2023-05-02T05:10:06.152149Z","shell.execute_reply":"2023-05-02T05:10:06.151079Z","shell.execute_reply.started":"2023-05-02T05:10:05.386935Z"},"trusted":true},"outputs":[],"source":["from keras.losses import CategoricalCrossentropy\n","from keras.callbacks import EarlyStopping\n","from keras.models import load_model\n","from keras.utils import plot_model\n","from keras.layers import Embedding\n","from keras.optimizers import Adam\n","from keras.layers import Dropout\n","from keras.models import Model\n","from keras.layers import Input\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Add\n","import keras_tuner\n","\n","with strategy.scope():  \n","    \n","    class HyperParamTunerModel(keras_tuner.HyperModel):\n","        \n","        def __init__(self, max_length, vocab_size):\n","            \n","            super(HyperParamTunerModel, self).__init__()\n","            \n","            self.max_length = max_length\n","            self.vocab_size = vocab_size\n","            \n","        # define the model\n","        def build(self, hp):\n","\n","            # features from the CNN model squeezed from 2048 to 256 nodes    \n","\n","            image_extract = Input(shape = (2048,), name = 'np_image')\n","            dence_units = hp.Int(\"dence_units\", min_value = 256, max_value = 1024, step = 32)\n","            \n","            fe1 = Dropout(hp.Float(\"img_dropout\", min_value = 0.5, max_value = 0.7, step = 0.05))(image_extract)\n","            fe2 = Dense(units = dence_units, activation = 'relu')(fe1)\n","\n","            # LSTM sequence model\n","            inputs_caption = Input(shape = (self.max_length,), name = 'word_seq')\n","\n","            se1 = Embedding(self.vocab_size, \n","                            hp.Int(\"emb_units\", min_value = 256, max_value = 1024, step = 32), \n","                            mask_zero = True)(inputs_caption)\n","            se2 = Dropout(hp.Float(\"emb_dropout\", min_value = 0.5, max_value = 0.7, step = 0.05))(se1)\n","            se3 = LSTM(dence_units)(se2)\n","\n","            # Merging both models\n","            decoder1 = Add()([fe2, se3])\n","            decoder2 = Dense(hp.Int(\"merge_units\", min_value = 256, max_value = 1024, step = 32), activation = 'relu')(decoder1)\n","\n","            outputs = Dense(self.vocab_size, activation = 'softmax', name = 'output_seq')(decoder2)\n","\n","            # tie it together [image, seq] [word]\n","            model = Model(inputs = [image_extract, inputs_caption], outputs = outputs)\n","\n","            #define optimizers\n","            lr = hp.Float(\"lr\", min_value = 1e-4, max_value = 1e-1, sampling = \"log\")#lr = 0.0001 \n","            lr = lr * num_replicas_in_sync\n","            adam_optimizers  = Adam(learning_rate = lr)\n","\n","            #define loss\n","            entropy_loss = CategoricalCrossentropy(from_logits = False)\n","\n","            model.compile(loss = entropy_loss, optimizer = adam_optimizers)\n","            return model\n","        \n","        \n","        def fit(self, hp, model, *args, **kwargs):\n","            \n","            return model.fit(\n","                            *args,\n","                            # Tune whether to shuffle the data in each epoch.\n","                            **kwargs,\n","                        )     \n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T05:10:06.156294Z","iopub.status.busy":"2023-05-02T05:10:06.155853Z","iopub.status.idle":"2023-05-02T05:10:07.332795Z","shell.execute_reply":"2023-05-02T05:10:07.331686Z","shell.execute_reply.started":"2023-05-02T05:10:06.156249Z"},"trusted":true},"outputs":[],"source":["tuner = keras_tuner.RandomSearch(\n","                                hypermodel = HyperParamTunerModel(max_in_seq_len, vocab_size),\n","                                objective = \"val_loss\",\n","                                max_trials = 10,\n","                                seed = 44,\n","                                executions_per_trial = 1,\n","                                distribution_strategy = strategy,\n","                                overwrite = True,\n","                                directory = os.path.join(PROJECT_ROOT, \"data\", \"hyper_param_search_result\"),\n","                                project_name = \"image_caption_hyper_param_search\",\n","                            )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T05:10:07.334530Z","iopub.status.busy":"2023-05-02T05:10:07.334156Z","iopub.status.idle":"2023-05-02T10:33:36.095681Z","shell.execute_reply":"2023-05-02T10:33:36.090690Z","shell.execute_reply.started":"2023-05-02T05:10:07.334491Z"},"trusted":true},"outputs":[],"source":["#Start the search\n","import math\n","\n","tuner.search(x = train_batch,\n","             steps_per_epoch = math.ceil(train_record_cnt/batch_size),\n","             epochs = 25,\n","             shuffle = False,\n","             verbose = 1,\n","             validation_data = valid_batch,\n","             validation_steps = math.ceil(valid_record_cnt/batch_size),\n","             callbacks = [EarlyStopping(monitor = 'val_loss', patience = 1)],\n","            )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T10:40:29.799950Z","iopub.status.busy":"2023-05-02T10:40:29.799512Z","iopub.status.idle":"2023-05-02T10:40:29.805588Z","shell.execute_reply":"2023-05-02T10:40:29.804112Z","shell.execute_reply.started":"2023-05-02T10:40:29.799912Z"},"trusted":true},"outputs":[],"source":["best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T10:44:19.704407Z","iopub.status.busy":"2023-05-02T10:44:19.703619Z","iopub.status.idle":"2023-05-02T10:44:19.711106Z","shell.execute_reply":"2023-05-02T10:44:19.709849Z","shell.execute_reply.started":"2023-05-02T10:44:19.704367Z"},"trusted":true},"outputs":[],"source":["for h_param in ['dence_units', 'img_dropout', 'emb_units', 'emb_dropout', 'merge_units', 'lr']:\n","      print(h_param, tuner.get_best_hyperparameters()[0].get(h_param))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T10:34:55.712374Z","iopub.status.busy":"2023-05-02T10:34:55.711402Z","iopub.status.idle":"2023-05-02T10:37:09.908388Z","shell.execute_reply":"2023-05-02T10:37:09.907295Z","shell.execute_reply.started":"2023-05-02T10:34:55.712334Z"},"trusted":true},"outputs":[],"source":["# Zip Hyperparamter output results. \n","# When download output from kaggel it was easier\n","\n","\"\"\"\n","import zipfile\n","\n","def zipdir(path, ziph):\n","    # ziph is zipfile handle\n","    for root, dirs, files in os.walk(path):\n","        for file in files:\n","            ziph.write(os.path.join(root, file))\n","    \n","zip_output_path = os.path.join(PROJECT_ROOT, \"data\", \"hyper_param_search_result.zip\")\n","zip_folder_path = os.path.join(PROJECT_ROOT, \"data\", \"hyper_param_search_result\")\n","zipf = zipfile.ZipFile(zip_output_path, 'w', zipfile.ZIP_DEFLATED)\n","zipdir(zip_folder_path, zipf)\n","zipf.close()  \n","\n","#!cd '/kaggle/working'\n","#!ls\n","##Output url link to download\n","#from IPython.display import FileLink\n","#FileLink(r'hyper_param_search_result.zip')\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["#### Hyperparamter Tuning Results: ####\n","\n","<table>\n","    <tr>\n","        <td>Trial</td>\n","        <td>dence_units</td>\n","        <td>img_dropout</td>\n","        <td>emb_units</td>\n","        <td>emb_dropout</td>\n","        <td>merge_units</td>\n","        <td>lr</td>\n","        <td>val_loss</td>\n","    </tr>\n","    <tr>\n","        <td>0</td>\n","        <td>576</td>\n","        <td>0.55</td>\n","        <td>928</td>\n","        <td>0.55</td>\n","        <td>672</td>\n","        <td>0.00015872740971656704</td>\n","        <td>4.634195327758789</td>\n","    </tr>\n","    <tr>\n","        <td>1</td>\n","        <td>640</td>\n","        <td>0.55</td>\n","        <td>992</td>\n","        <td>0.6</td>\n","        <td>960</td>\n","        <td>0.0001866463763857711</td>\n","        <td>4.594254016876221</td>\n","    </tr>\n","    <tr>\n","        <td>2</td>\n","        <td>992</td>\n","        <td>0.5</td>\n","        <td>704</td>\n","        <td>0.55</td>\n","        <td>480</td>\n","        <td>0.003044123781552541</td>\n","        <td>4.705258369445801</td>\n","    </tr>\n","    <tr>\n","        <td>3</td>\n","        <td>960</td>\n","        <td>0.6</td>\n","        <td>608</td>\n","        <td>0.60</td>\n","        <td>320</td>\n","        <td>0.00016764139674983655</td>\n","        <td>4.707552909851074</td>\n","    </tr>\n","    <tr>\n","        <td>4</td>\n","        <td>832</td>\n","        <td>0.65</td>\n","        <td>960</td>\n","        <td>0.55</td>\n","        <td>320</td>\n","        <td>0.0006909496810943752</td>\n","        <td>4.562685012817383</td>\n","    </tr>\n","    <tr>\n","        <td>5</td>\n","        <td>928</td>\n","        <td>0.60</td>\n","        <td>544</td>\n","        <td>0.65</td>\n","        <td>896</td>\n","        <td>0.0002729031103694528</td>\n","        <td>4.625041961669922</td>\n","    </tr>\n","    <tr>\n","        <td>6</td>\n","        <td>480</td>\n","        <td>0.60</td>\n","        <td>352</td>\n","        <td>0.60</td>\n","        <td>832</td>\n","        <td>0.0003969777071876592</td>\n","        <td>4.620487213134766</td>\n","    </tr>\n","    <tr>\n","        <td>7</td>\n","        <td>864</td>\n","        <td>0.55</td>\n","        <td>576</td>\n","        <td>0.5</td>\n","        <td>416</td>\n","        <td>0.0001780580297315219</td>\n","        <td>4.705492973327637</td>\n","    </tr>    \n","    <tr>\n","        <td>8</td>\n","        <td>576</td>\n","        <td>0.70</td>\n","        <td>672</td>\n","        <td>0.65</td>\n","        <td>544</td>\n","        <td>0.00038336436721221595</td>\n","        <td>4.599141597747803</td>\n","    </tr>\n","    <tr>\n","        <td>9</td>\n","        <td>544</td>\n","        <td>0.60</td>\n","        <td>352</td>\n","        <td>0.60</td>\n","        <td>384</td>\n","        <td>0.08652674373905335</td>\n","        <td>6.403066635131836</td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{},"source":["**Observation:**\n","\n","Top 2 paramaters where validation loss is less.\n","\n","<table>\n","    <tr>\n","        <td>Trial</td>\n","        <td>dence_units</td>\n","        <td>img_dropout</td>\n","        <td>emb_units</td>\n","        <td>emb_dropout</td>\n","        <td>merge_units</td>\n","        <td>lr</td>\n","        <td>val_loss</td>\n","    </tr>\n","    <tr>\n","        <td>1</td>\n","        <td>640</td>\n","        <td>0.55</td>\n","        <td>992</td>\n","        <td>0.6</td>\n","        <td>960</td>\n","        <td>0.0001866463763857711</td>\n","        <td>4.594254016876221</td>\n"," </tr>\n","<tr>\n","        <td>4</td>\n","        <td>832</td>\n","        <td>0.65</td>\n","        <td>960</td>\n","        <td>0.55</td>\n","        <td>320</td>\n","        <td>0.0006909496810943752</td>\n","        <td>4.562685012817383</td>\n"," </tr>\n","</table>\n","\n","Let's continue to test futher with more epoch the result of validation for more epoch\n"]},{"cell_type":"markdown","metadata":{},"source":["## Training ##\n","Train futher with top 3 best paramaters \n","\n","### Model ###"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-05-04T03:21:30.467367Z","iopub.status.busy":"2023-05-04T03:21:30.466448Z","iopub.status.idle":"2023-05-04T03:21:30.482151Z","shell.execute_reply":"2023-05-04T03:21:30.480977Z","shell.execute_reply.started":"2023-05-04T03:21:30.467328Z"},"trusted":true},"outputs":[],"source":["from keras.losses import CategoricalCrossentropy\n","from keras.callbacks import EarlyStopping\n","from keras.models import load_model\n","from keras.utils import plot_model\n","from keras.layers import Embedding\n","from keras.optimizers import Adam\n","from keras.layers import Dropout\n","from keras.models import Model\n","from keras.layers import Input\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Add\n","import math\n","\n","with strategy.scope():  \n","    \n","    def define_model(max_length, vocab_size, dence_units, img_dropout, emb_units, emb_dropout, merge_units, lr):\n","        \n","        image_extract = Input(shape = (2048,), name = 'np_image')\n","\n","        fe1 = Dropout(img_dropout)(image_extract)\n","        fe2 = Dense(units = dence_units, activation = 'relu')(fe1)\n","\n","        # LSTM sequence model\n","        inputs_caption = Input(shape = (max_length,), name = 'word_seq')\n","\n","        se1 = Embedding(vocab_size, \n","                        emb_units, \n","                        mask_zero = True)(inputs_caption)\n","        se2 = Dropout(emb_dropout)(se1)\n","        se3 = LSTM(dence_units)(se2)\n","\n","        # Merging both models\n","        decoder1 = Add()([fe2, se3])\n","        decoder2 = Dense(merge_units, activation = 'relu')(decoder1)\n","\n","        outputs = Dense(vocab_size, activation = 'softmax', name = 'output_seq')(decoder2)\n","\n","        # tie it together [image, seq] [word]\n","        model = Model(inputs = [image_extract, inputs_caption], outputs = outputs)\n","\n","        #define optimizers\n","        #lr = lr * num_replicas_in_sync\n","        adam_optimizers  = Adam(learning_rate = lr)\n","\n","        #define loss\n","        entropy_loss = CategoricalCrossentropy(from_logits = False)\n","\n","        model.compile(loss = entropy_loss, optimizer = adam_optimizers)\n","        \n","        return model"]},{"cell_type":"markdown","metadata":{},"source":["* **Trial: 1**\n","    * dence_units: 640 \n","    * img_dropout: 0.55\n","    * emb_units: 992\n","    * emb_dropout: 0.6 \n","    * merge_units: 960\n","    * lr: 0.0001866463763857711"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-05-03T10:32:42.479156Z","iopub.status.busy":"2023-05-03T10:32:42.477709Z","iopub.status.idle":"2023-05-03T12:01:41.990603Z","shell.execute_reply":"2023-05-03T12:01:41.989528Z","shell.execute_reply.started":"2023-05-03T10:32:42.479118Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/25\n","32/32 [==============================] - 247s 7s/step - loss: 7.4106 - val_loss: 6.4063\n","Epoch 2/25\n","32/32 [==============================] - 222s 7s/step - loss: 6.0803 - val_loss: 5.9783\n","Epoch 3/25\n","32/32 [==============================] - 215s 7s/step - loss: 5.6532 - val_loss: 5.5655\n","Epoch 4/25\n","32/32 [==============================] - 207s 7s/step - loss: 5.2634 - val_loss: 5.2760\n","Epoch 5/25\n","32/32 [==============================] - 209s 7s/step - loss: 4.9177 - val_loss: 5.0241\n","Epoch 6/25\n","32/32 [==============================] - 215s 7s/step - loss: 4.6486 - val_loss: 4.8930\n","Epoch 7/25\n","32/32 [==============================] - 207s 7s/step - loss: 4.4682 - val_loss: 4.8175\n","Epoch 8/25\n","32/32 [==============================] - 244s 8s/step - loss: 4.3210 - val_loss: 4.7692\n","Epoch 9/25\n","32/32 [==============================] - 226s 7s/step - loss: 4.1872 - val_loss: 4.7361\n","Epoch 10/25\n","32/32 [==============================] - 209s 7s/step - loss: 4.0717 - val_loss: 4.7160\n","Epoch 11/25\n","32/32 [==============================] - 175s 6s/step - loss: 3.9625 - val_loss: 4.7051\n","Epoch 12/25\n","32/32 [==============================] - 214s 7s/step - loss: 3.8595 - val_loss: 4.6940\n","Epoch 13/25\n","32/32 [==============================] - 208s 7s/step - loss: 3.7652 - val_loss: 4.6877\n","Epoch 14/25\n","32/32 [==============================] - 214s 7s/step - loss: 3.6740 - val_loss: 4.6886\n","Epoch 15/25\n","32/32 [==============================] - 244s 8s/step - loss: 3.5880 - val_loss: 4.7050\n","Epoch 16/25\n","32/32 [==============================] - 174s 6s/step - loss: 3.5076 - val_loss: 4.7077\n","Epoch 17/25\n","32/32 [==============================] - 206s 7s/step - loss: 3.4282 - val_loss: 4.7317\n","Epoch 18/25\n","32/32 [==============================] - 212s 7s/step - loss: 3.3545 - val_loss: 4.7510\n","Epoch 19/25\n","32/32 [==============================] - 247s 8s/step - loss: 3.2819 - val_loss: 4.7724\n","Epoch 20/25\n","32/32 [==============================] - 210s 7s/step - loss: 3.2133 - val_loss: 4.8011\n","Epoch 21/25\n","32/32 [==============================] - 208s 7s/step - loss: 3.1434 - val_loss: 4.8229\n","Epoch 22/25\n","32/32 [==============================] - 191s 6s/step - loss: 3.0828 - val_loss: 4.8533\n","Epoch 23/25\n","32/32 [==============================] - 216s 7s/step - loss: 3.0207 - val_loss: 4.8811\n","Epoch 24/25\n","32/32 [==============================] - 243s 8s/step - loss: 2.9623 - val_loss: 4.9135\n","Epoch 25/25\n","32/32 [==============================] - 172s 6s/step - loss: 2.9075 - val_loss: 4.9340\n"]}],"source":["#define model\n","train_model = define_model(max_in_seq_len, vocab_size,\n","                           dence_units = 640 , \n","                           img_dropout = 0.55 , \n","                           emb_units = 992, \n","                           emb_dropout = 0.6, \n","                           merge_units = 960, \n","                           lr = 0.0001866463763857711)\n","\n","history = train_model.fit(x = train_batch,\n","                          steps_per_epoch = math.ceil(train_record_cnt/batch_size),\n","                          epochs = 25,\n","                          shuffle = False,\n","                          verbose = 1,\n","                          validation_data = valid_batch,\n","                          validation_steps = math.ceil(valid_record_cnt/batch_size),             \n","                       )"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-05-03T12:02:06.359322Z","iopub.status.busy":"2023-05-03T12:02:06.358933Z","iopub.status.idle":"2023-05-03T12:02:06.392690Z","shell.execute_reply":"2023-05-03T12:02:06.391707Z","shell.execute_reply.started":"2023-05-03T12:02:06.359284Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","hist_df = pd.DataFrame(history.history) \n","\n","hist_df.to_parquet(os.path.join(PROJECT_ROOT, 'data', 'encoder-decoder-neural network', 'result_dence_units_640,img_dropout_55,emb_units_992,emb_dropout_60,merge_units_960,lr_0_0001866463763857711.parquet'))"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-05-03T12:02:13.219879Z","iopub.status.busy":"2023-05-03T12:02:13.219351Z","iopub.status.idle":"2023-05-03T12:02:13.239710Z","shell.execute_reply":"2023-05-03T12:02:13.238509Z","shell.execute_reply.started":"2023-05-03T12:02:13.219830Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>loss</th>\n","      <th>val_loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>7.410568</td>\n","      <td>6.406340</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>6.080339</td>\n","      <td>5.978259</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5.653186</td>\n","      <td>5.565533</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5.263391</td>\n","      <td>5.275986</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4.917656</td>\n","      <td>5.024098</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       loss  val_loss\n","0  7.410568  6.406340\n","1  6.080339  5.978259\n","2  5.653186  5.565533\n","3  5.263391  5.275986\n","4  4.917656  5.024098"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["hist_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["**Observation:**\n","\n","### Model is overfitting on train data ###"]},{"cell_type":"markdown","metadata":{},"source":["* **Trial: 2**\n","    * dence_units: 832 \n","    * img_dropout: 0.65\n","    * emb_units: 960\n","    * emb_dropout: 0.55 \n","    * merge_units: 320\n","    * lr: 0.0006909496810943752"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-05-04T03:21:49.999268Z","iopub.status.busy":"2023-05-04T03:21:49.998257Z","iopub.status.idle":"2023-05-04T06:16:06.097098Z","shell.execute_reply":"2023-05-04T06:16:06.096017Z","shell.execute_reply.started":"2023-05-04T03:21:49.999227Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","32/32 [==============================] - 278s 8s/step - loss: 6.7806 - val_loss: 6.0437\n","Epoch 2/50\n","32/32 [==============================] - 222s 7s/step - loss: 5.6377 - val_loss: 5.4305\n","Epoch 3/50\n","32/32 [==============================] - 212s 7s/step - loss: 4.9936 - val_loss: 4.9648\n","Epoch 4/50\n","32/32 [==============================] - 205s 7s/step - loss: 4.5646 - val_loss: 4.7630\n","Epoch 5/50\n","32/32 [==============================] - 204s 7s/step - loss: 4.2858 - val_loss: 4.6591\n","Epoch 6/50\n","32/32 [==============================] - 240s 8s/step - loss: 4.0720 - val_loss: 4.6094\n","Epoch 7/50\n","32/32 [==============================] - 185s 6s/step - loss: 3.9000 - val_loss: 4.6168\n","Epoch 8/50\n","32/32 [==============================] - 204s 7s/step - loss: 3.7533 - val_loss: 4.5877\n","Epoch 9/50\n","32/32 [==============================] - 205s 7s/step - loss: 3.6015 - val_loss: 4.6248\n","Epoch 10/50\n","32/32 [==============================] - 202s 7s/step - loss: 3.4620 - val_loss: 4.6664\n","Epoch 11/50\n","32/32 [==============================] - 238s 8s/step - loss: 3.3370 - val_loss: 4.7207\n","Epoch 12/50\n","32/32 [==============================] - 169s 5s/step - loss: 3.2162 - val_loss: 4.7898\n","Epoch 13/50\n","32/32 [==============================] - 239s 8s/step - loss: 3.1047 - val_loss: 4.8589\n","Epoch 14/50\n","32/32 [==============================] - 206s 7s/step - loss: 3.0169 - val_loss: 4.9059\n","Epoch 15/50\n","32/32 [==============================] - 173s 6s/step - loss: 2.9303 - val_loss: 4.9632\n","Epoch 16/50\n","32/32 [==============================] - 208s 7s/step - loss: 2.8337 - val_loss: 5.0478\n","Epoch 17/50\n","32/32 [==============================] - 207s 7s/step - loss: 2.7149 - val_loss: 5.1388\n","Epoch 18/50\n","32/32 [==============================] - 205s 7s/step - loss: 2.6138 - val_loss: 5.2238\n","Epoch 19/50\n","32/32 [==============================] - 239s 8s/step - loss: 2.5262 - val_loss: 5.3127\n","Epoch 20/50\n","32/32 [==============================] - 169s 5s/step - loss: 2.4514 - val_loss: 5.3565\n","Epoch 21/50\n","32/32 [==============================] - 204s 7s/step - loss: 2.3782 - val_loss: 5.3797\n","Epoch 22/50\n","32/32 [==============================] - 239s 8s/step - loss: 2.3140 - val_loss: 5.4313\n","Epoch 23/50\n","32/32 [==============================] - 170s 5s/step - loss: 2.2391 - val_loss: 5.5039\n","Epoch 24/50\n","32/32 [==============================] - 207s 7s/step - loss: 2.1656 - val_loss: 5.5631\n","Epoch 25/50\n","32/32 [==============================] - 204s 7s/step - loss: 2.0975 - val_loss: 5.6381\n","Epoch 26/50\n","32/32 [==============================] - 208s 7s/step - loss: 2.0433 - val_loss: 5.6893\n","Epoch 27/50\n","32/32 [==============================] - 212s 7s/step - loss: 1.9835 - val_loss: 5.7538\n","Epoch 28/50\n","32/32 [==============================] - 242s 8s/step - loss: 1.9244 - val_loss: 5.8347\n","Epoch 29/50\n","32/32 [==============================] - 205s 7s/step - loss: 1.8713 - val_loss: 5.9084\n","Epoch 30/50\n","32/32 [==============================] - 171s 5s/step - loss: 1.8251 - val_loss: 5.9733\n","Epoch 31/50\n","32/32 [==============================] - 208s 7s/step - loss: 1.7804 - val_loss: 6.0189\n","Epoch 32/50\n","32/32 [==============================] - 214s 7s/step - loss: 1.7479 - val_loss: 6.0591\n","Epoch 33/50\n","32/32 [==============================] - 205s 7s/step - loss: 1.7195 - val_loss: 6.0933\n","Epoch 34/50\n","32/32 [==============================] - 206s 7s/step - loss: 1.6892 - val_loss: 6.1677\n","Epoch 35/50\n","32/32 [==============================] - 206s 7s/step - loss: 1.6331 - val_loss: 6.2414\n","Epoch 36/50\n","32/32 [==============================] - 243s 8s/step - loss: 1.5764 - val_loss: 6.3230\n","Epoch 37/50\n","32/32 [==============================] - 177s 6s/step - loss: 1.5251 - val_loss: 6.3722\n","Epoch 38/50\n","32/32 [==============================] - 208s 7s/step - loss: 1.4815 - val_loss: 6.4309\n","Epoch 39/50\n","32/32 [==============================] - 209s 7s/step - loss: 1.4491 - val_loss: 6.4916\n","Epoch 40/50\n","32/32 [==============================] - 206s 7s/step - loss: 1.4152 - val_loss: 6.5391\n","Epoch 41/50\n","32/32 [==============================] - 209s 7s/step - loss: 1.3863 - val_loss: 6.5922\n","Epoch 42/50\n","32/32 [==============================] - 212s 7s/step - loss: 1.3533 - val_loss: 6.6501\n","Epoch 43/50\n","32/32 [==============================] - 206s 7s/step - loss: 1.3298 - val_loss: 6.7137\n","Epoch 44/50\n","32/32 [==============================] - 240s 8s/step - loss: 1.3053 - val_loss: 6.7714\n","Epoch 45/50\n","32/32 [==============================] - 167s 5s/step - loss: 1.2787 - val_loss: 6.7880\n","Epoch 46/50\n","32/32 [==============================] - 206s 7s/step - loss: 1.2540 - val_loss: 6.8107\n","Epoch 47/50\n","32/32 [==============================] - 208s 7s/step - loss: 1.2267 - val_loss: 6.8397\n","Epoch 48/50\n","32/32 [==============================] - 242s 8s/step - loss: 1.2006 - val_loss: 6.8854\n","Epoch 49/50\n","32/32 [==============================] - 169s 5s/step - loss: 1.1777 - val_loss: 6.8999\n","Epoch 50/50\n","32/32 [==============================] - 241s 8s/step - loss: 1.1489 - val_loss: 6.9065\n"]}],"source":["#define model\n","train_model = define_model(max_in_seq_len, vocab_size,\n","                           dence_units = 832 , \n","                           img_dropout = 0.65 , \n","                           emb_units = 960, \n","                           emb_dropout = 0.55, \n","                           merge_units = 320, \n","                           lr = 0.0006909496810943752)\n","\n","history = train_model.fit(x = train_batch,\n","                          steps_per_epoch = math.ceil(train_record_cnt/batch_size),\n","                          epochs = 50,\n","                          shuffle = False,\n","                          verbose = 1,\n","                          validation_data = valid_batch,\n","                          validation_steps = math.ceil(valid_record_cnt/batch_size),             \n","                       )"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-05-04T06:17:47.169251Z","iopub.status.busy":"2023-05-04T06:17:47.168534Z","iopub.status.idle":"2023-05-04T06:17:47.182939Z","shell.execute_reply":"2023-05-04T06:17:47.181922Z","shell.execute_reply.started":"2023-05-04T06:17:47.169211Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","hist_df = pd.DataFrame(history.history) \n","\n","hist_df.to_parquet(os.path.join(PROJECT_ROOT, 'data', 'encoder-decoder-neural network', 'result_dence_units_832,img_dropout_65,emb_units_960,emb_dropout_55,merge_units_320,lr_0_0006909496810943752.parquet'))"]},{"cell_type":"markdown","metadata":{},"source":["**Observation:**\n","\n","### Model is overfitting on train data ###"]},{"cell_type":"markdown","metadata":{},"source":["* **Trial: 3**\n","    * dence_units: 576 \n","    * img_dropout: 0.70\n","    * emb_units: 672\n","    * emb_dropout: 0.65\n","    * merge_units: 544\n","    * lr: 0.00038336436721221595"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#define model\n","train_model = define_model(max_in_seq_len, vocab_size,\n","                           dence_units = 576 , \n","                           img_dropout = 0.70 , \n","                           emb_units = 672, \n","                           emb_dropout = 0.65, \n","                           merge_units = 544, \n","                           lr = 0.00038336436721221595)\n","\n","history = train_model.fit(x = train_batch,\n","                          steps_per_epoch = math.ceil(train_record_cnt/batch_size),\n","                          epochs = 50,\n","                          shuffle = False,\n","                          verbose = 1,\n","                          validation_data = valid_batch,\n","                          validation_steps = math.ceil(valid_record_cnt/batch_size),             \n","                       )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","\n","hist_df = pd.DataFrame(history.history) \n","\n","hist_df.to_parquet(os.path.join(PROJECT_ROOT, 'data', 'encoder-decoder-neural network', 'result_dence_units_576,img_dropout_70,emb_units_672,emb_dropout_65,merge_units_544,lr_0_00038336436721221595.parquet'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_model.save(os.path.join(PROJECT_ROOT, 'data', 'encoder-decoder-neural network','my_model.h5'))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.8"}},"nbformat":4,"nbformat_minor":4}
