{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Caption Encoder and Decoder based model using Transfromer(ViT-BERT) # \n",
    "\n",
    "This involves two elements:\n",
    "\n",
    "1. Vision Transformers (ViT): Transformer based architecture that uses self-attention mechanisms to process images. Using it to extract image features\n",
    "2. Bidirectional Encoder Representations from Transformers(BERT): It's Large Language Model(LLM) for text data\n",
    "\n",
    "Help reference on how to train transfromer model from scratch: </br>\n",
    "i)   https://sushantjha8.medium.com/lets-train-image-to-text-transformer-846150b632ef </br>\n",
    "ii)  https://medium.com/nlplanet/bert-finetuning-with-hugging-face-and-training-visualizations-with-tensorboard-46368a57fc97 </br>\n",
    "iii) https://ankur3107.github.io/blogs/the-illustrated-image-captioning-using-transformers/ </br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils as plh\n",
    "\n",
    "PROJECT_ROOT = plh.get_project_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processor steps for the image-text data #\n",
    "\n",
    "Define object to pre-process the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process: Image using ViTImageProcessor ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor #preprocessing the input image that will be given as input to model VisionEncoderDecoderModel\n",
    "\n",
    "image_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\") #Image processed that was proposed in the original papper by google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process: Text using BertTokenizer ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer #generated target tokens to the target string\n",
    "\n",
    "# VisionEncoderDecoderModel require to define eos_token_id, decoder_start_token_id , pad_token_id  in the model conifg. \n",
    "# When missing these configuration in model.config gave error \"Make sure to set the decoder_start_token_id attribute of the model's configuration.\" during training\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PAD Token\n",
    "(tokenizer.pad_token, tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLS Token\n",
    "(tokenizer.cls_token, tokenizer.cls_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEP Token\n",
    "(tokenizer.sep_token, tokenizer.sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# By default \"bert-base-uncased\" config does not return bos_token_id, eos_token_id\n",
    "# use BERT's [cls] token as beginning of the sentence (BOS) token and [sep] token as end of the sentence (EOS) token. \n",
    "# Got this idea solution from the help doc: https://huggingface.co/transformers/v3.3.1/model_doc/bertgeneration.html#bertgenerationtokenizer\n",
    "# Setting bos_token and bos_token_id from from_pretrained did not help\n",
    "\n",
    "tokenizer.bos_token = tokenizer.cls_token\n",
    "tokenizer.bos_token_id = tokenizer.cls_token_id\n",
    "\n",
    "print('BOS:' + tokenizer.bos_token, tokenizer.bos_token_id)\n",
    "\n",
    "\n",
    "tokenizer.eos_token = tokenizer.sep_token\n",
    "tokenizer.eos_token_id = tokenizer.sep_token_id\n",
    "\n",
    "print('EOS:' + tokenizer.eos_token, tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model #\n",
    "\n",
    "Define the config of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "config_decoder = BertConfig() #Load the based architecture/configuration of Bert model\n",
    "config_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import ViTConfig\n",
    "\n",
    "config_encoder = ViTConfig() #Load the based architecture/configuration of ViT model\n",
    "config_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderConfig\n",
    "\n",
    "config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder) #Create config Vit-Bert using the configuration of ViT and Bert\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Tensorflow\n",
    "#from transformers import TFVisionEncoderDecoderModel #Load tensorflow based VisionEncoderDecoderModel. Model to generate text from image\n",
    "#model = TFVisionEncoderDecoderModel(config = config) #Build the model using the config\n",
    "\n",
    "#For pytorch\n",
    "from transformers import VisionEncoderDecoderModel\n",
    "model = VisionEncoderDecoderModel(config = config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the model config with the token ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tokenizer.pad_token , tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tokenizer.pad_token_id , tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "#model.config.decoder.eos_token_id = tokenizer.eos_token_id\n",
    "#model.config.decoder.decoder_start_token_id = tokenizer.bos_token_id\n",
    "#model.config.decoder.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "#model.config.encoder.eos_token_id = tokenizer.eos_token_id\n",
    "#model.config.encoder.decoder_start_token_id = tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://discuss.huggingface.co/t/error-training-vision-encoder-decoder-for-image-captioning/12090/6\n",
    "\n",
    "#model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data #\n",
    "\n",
    "Load the data from parquet file into dataset object and define class to apply pre-process on the data's.\n",
    "\n",
    "HuggingFace dataset: https://huggingface.co/docs/datasets/loading#parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "\n",
    "#Load from parquet file into huggingface dataset\n",
    "base_path = os.path.join(PROJECT_ROOT, 'data', 'processed')\n",
    "train_data = 'train_data_processed.parquet'\n",
    "valid_data = 'validate_data_processed.parquet'\n",
    "\n",
    "data_files = {\"train\": os.path.join(base_path, train_data), 'valid': os.path.join(base_path, valid_data)}\n",
    "db_set = load_dataset(\"parquet\", data_files = data_files)\n",
    "\n",
    "db_set = db_set.remove_columns(['id', 'title', 'color', 'clean_title', 'clean_color', '__index_level_0__'])\n",
    "\n",
    "db_set = db_set.map(lambda dbrow: { \"image_path\": os.path.join(PROJECT_ROOT, 'data', 'images', dbrow[\"image_name\"]) }, remove_columns = [\"image_name\"])\n",
    "\n",
    "db_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_set['train'].num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_set['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at top 5 records in the train dataset\n",
    "db_set['train'][\"image_path\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the pre-process on image and text on the dataset\n",
    "#import tensorflow as tf\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class captiondatset(Dataset):\n",
    "\n",
    "    def __init__(self, datasets, length):\n",
    "        self.datasets = datasets\n",
    "        self.length = length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = Image.open(self.datasets['image_path'][idx])\n",
    "        image_features = image_processor(image, return_tensors = \"pt\").pixel_values #tf\n",
    "\n",
    "        labels = tokenizer(self.datasets[\"caption\"][idx], \n",
    "                           return_tensors = \"pt\", #tf\n",
    "                           max_length = 15,\n",
    "                           padding = 'max_length',\n",
    "                           return_token_type_ids = True,\n",
    "                           truncation = True).input_ids\n",
    "\n",
    "        #return {'pixel_values': tf.squeeze(image_features), 'labels': tf.squeeze(labels)}\n",
    "        return {'pixel_values': image_features.squeeze(0), 'labels': labels.squeeze(0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datsets_train = captiondatset(db_set['train'], db_set['train'].num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the transformed output at row idx 0\n",
    "datsets_train.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datsets_train.__getitem__(16)['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val = captiondatset(db_set['valid'], db_set['valid'].num_rows) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "del [db_set]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate #\n",
    "\n",
    "Define evaluate metric for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer, scoring\n",
    "\n",
    "rouge_types = [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    "use_stemmer = False\n",
    "\n",
    "rouge_score_obj = rouge_scorer.RougeScorer(rouge_types = rouge_types, use_stemmer = use_stemmer)\n",
    "aggregator = scoring.BootstrapAggregator()\n",
    "\n",
    "def compute_metrics(pred):\n",
    "\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens = True)\n",
    "\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens = True)\n",
    "\n",
    "    for ref, pred in zip(label_str, pred_str):\n",
    "\n",
    "        output_score = rouge_score_obj.score(prediction = pred, target = ref)\n",
    "        aggregator.add_scores(output_score)\n",
    "\n",
    "    result = aggregator.aggregate()\n",
    "    \n",
    "    return {\n",
    "        \"rouge1_fmeasure\": round(result['rouge1'].mid.fmeasure, 2),\n",
    "        \"rouge2_fmeasure\": round(result['rouge2'].mid.fmeasure, 2),\n",
    "        \"rougeL_fmeasure\": round(result['rougeL'].mid.fmeasure, 2),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Arguments ###\n",
    "\n",
    "Help Refer: https://huggingface.co/transformers/v3.5.1/main_classes/trainer.html#transformers.TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Where model checkpoints will be saved.\n",
    "output_dir = './data/vit_bert/'\n",
    "\n",
    "\n",
    "# evaluating the trained model on the evaluation set every eval_steps training steps \n",
    "# evaluation_strategy (default \"no\"):\n",
    "# Possible values are:\n",
    "# \"no\": No evaluation is done during training.\n",
    "# \"steps\": Evaluation is done (and logged) every eval_steps paramater.\n",
    "# \"epoch\": Evaluation is done at the end of each epoch.   \n",
    "evaluation_strategy = 'steps'\n",
    "eval_steps = 6\n",
    "\n",
    "\n",
    "# logging_strategy (default: \"steps\"): The logging strategy to adopt during. These writen training logs will be used by TensorBoard visualized.\n",
    "# training (used to log training loss for example). Possible values are:\n",
    "# \"no\": No logging is done during training.\n",
    "# \"epoch\": Logging is done at the end of each epoch.\n",
    "# \"steps\": Logging is done every logging_steps.\n",
    "logging_strategy = \"steps\"\n",
    "# logging_steps (default 500): Number of update steps between two logs if\n",
    "# logging_strategy=\"steps\".\n",
    "logging_steps = 5\n",
    "\n",
    "\n",
    "# Save the trained model.\n",
    "# The checkpoint save strategy to adopt during training. Possible values are:\n",
    "# \"no\": No save is done during training.\n",
    "# \"epoch\": Save is done at the end of each epoch.\n",
    "# \"steps\": Save is done every save_steps (default 500).\n",
    "save_strategy = \"epoch\"\n",
    "# save_steps (default: 500): Number of updates steps before two checkpoint\n",
    "# saves if save_strategy=\"steps\".\n",
    "#save_steps = 200\n",
    "\n",
    "\n",
    "learning_rate = 2e-4\n",
    "\n",
    "\n",
    "# per_device_train_batch_size: The batch size per GPU/TPU core/CPU for training.\n",
    "per_device_train_batch_size = 32 #4\n",
    "gradient_accumulation_steps = 16 #When use gradient_accumulation_steps with per_device_train_batch_size it has same effective batch size 64 when per_batch_size = 4 and grad_steps = 16\n",
    "\n",
    "# per_device_eval_batch_size: The batch size per GPU/TPU core/CPU for evaluation.\n",
    "per_device_eval_batch_size = 32\n",
    "\n",
    "\n",
    "fp16 = True\n",
    "\n",
    "\n",
    "# Total number of training epochs to perform\n",
    "num_train_epochs = 3\n",
    "    \n",
    "\n",
    "# load_best_model_at_end (default False): Whether or not to load the best model\n",
    "# found during training at the end of training.\n",
    "load_best_model_at_end = False\n",
    "\n",
    "\n",
    "# metric_for_best_model:\n",
    "# Use in conjunction with load_best_model_at_end to specify the metric to use\n",
    "# to compare two different models. Must be the name of a metric returned by\n",
    "# the evaluation with or without the prefix \"eval_\".\n",
    "metric_for_best_model = compute_metrics\n",
    " \n",
    "# report_to:\n",
    "# The list of integrations to report the results and logs to. Supported\n",
    "# platforms are \"azure_ml\", \"comet_ml\", \"mlflow\", \"tensorboard\" and \"wandb\".\n",
    "# Use \"all\" to report to all integrations installed, \"none\" for no integrations.\n",
    "report_to = None #\"tensorboard\". \n",
    "\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "                                output_dir = output_dir,         \n",
    "                                                                \n",
    "                                learning_rate = learning_rate,\n",
    "\n",
    "                                num_train_epochs = num_train_epochs,                  \n",
    "\n",
    "                                per_device_train_batch_size = per_device_train_batch_size,       \n",
    "                                gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "                                per_device_eval_batch_size = per_device_eval_batch_size,        \n",
    "                                \n",
    "                                fp16 = fp16,\n",
    "\n",
    "                                evaluation_strategy = evaluation_strategy,\n",
    "                                eval_steps = eval_steps,\n",
    "\n",
    "                                logging_strategy  = logging_strategy,\n",
    "                                #logging_dir = './data/logs',\n",
    "                                logging_steps  = logging_steps, \n",
    "\n",
    "                                save_strategy  = save_strategy,\n",
    "\n",
    "                                load_best_model_at_end = load_best_model_at_end,    \n",
    "\n",
    "                                report_to = report_to,                           \n",
    "                                \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start TensorBoard before training to monitor it in progress\n",
    "\n",
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir output_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ###\n",
    "\n",
    "Help refer: https://huggingface.co/docs/transformers/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "\n",
    "#For tensorflow\n",
    "\n",
    "#import tensorflow as tf\n",
    "#devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "#For Pytorch\n",
    "import torch\n",
    "\n",
    "devices = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = model.to(devices)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "from transformers import Trainer\n",
    "\n",
    "#For train tensorflow: using compile and fit methods of tf.\n",
    "\n",
    "#Train with PyTorch use Trainer Class Object\n",
    "trainer = Trainer(\n",
    "                model = model,                         # the instantiated Transformers model to be trained\n",
    "                args = training_args,                  # training arguments, defined above\n",
    "                train_dataset = datsets_train,         # training dataset\n",
    "                eval_dataset = dataset_val,             # evaluation dataset\n",
    "                compute_metrics = metric_for_best_model,\n",
    "                data_collator = default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_DISABLED'] = 'True' #When training in Kaggel it was getting connected to WANDB.io and was asking for API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a8019b6faff98163eb884b931cd42fce4cc159f4b6cbdadf8184d77ab18833d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
