{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Image Caption Encoder and Decoder based model using Transfromer(ViT-BERT) # \n","\n","This involves two elements:\n","\n","1. Vision Transformers (ViT): Transformer based architecture that uses self-attention mechanisms to process images. Using it to extract image features\n","2. Bidirectional Encoder Representations from Transformers(BERT): It's Large Language Model(LLM) for text data\n","\n","Help reference on how to train transfromer model from scratch: </br>\n","i)   https://sushantjha8.medium.com/lets-train-image-to-text-transformer-846150b632ef </br>\n","ii)  https://medium.com/nlplanet/bert-finetuning-with-hugging-face-and-training-visualizations-with-tensorboard-46368a57fc97 </br>\n","iii) https://ankur3107.github.io/blogs/the-illustrated-image-captioning-using-transformers/ </br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import src.utils as plh\n","\n","PROJECT_ROOT = plh.get_project_root()"]},{"cell_type":"markdown","metadata":{},"source":["# Pre-Processor steps for the image-text data #\n","\n","Define object to pre-process the inputs"]},{"cell_type":"markdown","metadata":{},"source":["## Pre-Process: Image using ViTImageProcessor ##"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import ViTImageProcessor #preprocessing the input image that will be given as input to model VisionEncoderDecoderModel\n","\n","image_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\") #Image processed that was proposed in the original papper by google"]},{"cell_type":"markdown","metadata":{},"source":["## Pre-Process: Text using BertTokenizer ##"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import BertTokenizer #generated target tokens to the target string\n","\n","# VisionEncoderDecoderModel require to define eos_token_id, decoder_start_token_id , pad_token_id  in the model conifg. \n","# When missing these configuration in model.config gave error \"Make sure to set the decoder_start_token_id attribute of the model's configuration.\" during training\n","\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#PAD Token\n","(tokenizer.pad_token, tokenizer.pad_token_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#CLS Token\n","(tokenizer.cls_token, tokenizer.cls_token_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#SEP Token\n","(tokenizer.sep_token, tokenizer.sep_token_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","# By default \"bert-base-uncased\" config does not return bos_token_id, eos_token_id\n","# use BERT's [cls] token as beginning of the sentence (BOS) token and [sep] token as end of the sentence (EOS) token. \n","# Got this idea solution from the help doc: https://huggingface.co/transformers/v3.3.1/model_doc/bertgeneration.html#bertgenerationtokenizer\n","# Setting bos_token and bos_token_id from from_pretrained did not help\n","\n","tokenizer.bos_token = tokenizer.cls_token\n","tokenizer.bos_token_id = tokenizer.cls_token_id\n","\n","print('BOS:' + tokenizer.bos_token, tokenizer.bos_token_id)\n","\n","\n","tokenizer.eos_token = tokenizer.sep_token\n","tokenizer.eos_token_id = tokenizer.sep_token_id\n","\n","print('EOS:' + tokenizer.eos_token, tokenizer.eos_token_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tokenizer"]},{"cell_type":"markdown","metadata":{},"source":["# Define Model #\n","\n","Define the config of the model"]},{"cell_type":"markdown","metadata":{},"source":["## Load for pre-saved checkpoint ##"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","\n","pre_trained_model_exists = False\n","\n","checkpoint = os.path.join(PROJECT_ROOT, 'vit-bert-based-checkpoint')\n","if os.path.exists(checkpoint):\n","    \n","    pre_trained_model_exists = True\n","    \n","    from transformers import VisionEncoderDecoderModel\n","    model = VisionEncoderDecoderModel.from_pretrained(checkpoint)\n","    \n","\n","pre_trained_model_exists"]},{"cell_type":"markdown","metadata":{},"source":["## Init Model ##"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-05-17T05:21:42.386761Z","iopub.status.busy":"2023-05-17T05:21:42.386040Z","iopub.status.idle":"2023-05-17T05:21:42.397716Z","shell.execute_reply":"2023-05-17T05:21:42.396640Z","shell.execute_reply.started":"2023-05-17T05:21:42.386710Z"},"trusted":true},"outputs":[{"data":{"text/plain":["BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.28.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import BertConfig\n","\n","config_decoder = BertConfig() #Load the based architecture/configuration of Bert model\n","config_decoder"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-05-17T05:21:42.400656Z","iopub.status.busy":"2023-05-17T05:21:42.399481Z","iopub.status.idle":"2023-05-17T05:21:42.415724Z","shell.execute_reply":"2023-05-17T05:21:42.414658Z","shell.execute_reply.started":"2023-05-17T05:21:42.400608Z"},"trusted":true},"outputs":[{"data":{"text/plain":["ViTConfig {\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"encoder_stride\": 16,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 768,\n","  \"image_size\": 224,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"model_type\": \"vit\",\n","  \"num_attention_heads\": 12,\n","  \"num_channels\": 3,\n","  \"num_hidden_layers\": 12,\n","  \"patch_size\": 16,\n","  \"qkv_bias\": true,\n","  \"transformers_version\": \"4.28.1\"\n","}"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import ViTConfig\n","\n","config_encoder = ViTConfig() #Load the based architecture/configuration of ViT model\n","config_encoder"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-05-17T05:21:42.418281Z","iopub.status.busy":"2023-05-17T05:21:42.417335Z","iopub.status.idle":"2023-05-17T05:21:42.434619Z","shell.execute_reply":"2023-05-17T05:21:42.433240Z","shell.execute_reply.started":"2023-05-17T05:21:42.418237Z"},"trusted":true},"outputs":[{"data":{"text/plain":["VisionEncoderDecoderConfig {\n","  \"_commit_hash\": null,\n","  \"decoder\": {\n","    \"_name_or_path\": \"\",\n","    \"add_cross_attention\": true,\n","    \"architectures\": null,\n","    \"attention_probs_dropout_prob\": 0.1,\n","    \"bad_words_ids\": null,\n","    \"begin_suppress_tokens\": null,\n","    \"bos_token_id\": null,\n","    \"chunk_size_feed_forward\": 0,\n","    \"classifier_dropout\": null,\n","    \"cross_attention_hidden_size\": null,\n","    \"decoder_start_token_id\": null,\n","    \"diversity_penalty\": 0.0,\n","    \"do_sample\": false,\n","    \"early_stopping\": false,\n","    \"encoder_no_repeat_ngram_size\": 0,\n","    \"eos_token_id\": null,\n","    \"exponential_decay_length_penalty\": null,\n","    \"finetuning_task\": null,\n","    \"forced_bos_token_id\": null,\n","    \"forced_eos_token_id\": null,\n","    \"hidden_act\": \"gelu\",\n","    \"hidden_dropout_prob\": 0.1,\n","    \"hidden_size\": 768,\n","    \"id2label\": {\n","      \"0\": \"LABEL_0\",\n","      \"1\": \"LABEL_1\"\n","    },\n","    \"initializer_range\": 0.02,\n","    \"intermediate_size\": 3072,\n","    \"is_decoder\": true,\n","    \"is_encoder_decoder\": false,\n","    \"label2id\": {\n","      \"LABEL_0\": 0,\n","      \"LABEL_1\": 1\n","    },\n","    \"layer_norm_eps\": 1e-12,\n","    \"length_penalty\": 1.0,\n","    \"max_length\": 20,\n","    \"max_position_embeddings\": 512,\n","    \"min_length\": 0,\n","    \"model_type\": \"bert\",\n","    \"no_repeat_ngram_size\": 0,\n","    \"num_attention_heads\": 12,\n","    \"num_beam_groups\": 1,\n","    \"num_beams\": 1,\n","    \"num_hidden_layers\": 12,\n","    \"num_return_sequences\": 1,\n","    \"output_attentions\": false,\n","    \"output_hidden_states\": false,\n","    \"output_scores\": false,\n","    \"pad_token_id\": 0,\n","    \"position_embedding_type\": \"absolute\",\n","    \"prefix\": null,\n","    \"problem_type\": null,\n","    \"pruned_heads\": {},\n","    \"remove_invalid_values\": false,\n","    \"repetition_penalty\": 1.0,\n","    \"return_dict\": true,\n","    \"return_dict_in_generate\": false,\n","    \"sep_token_id\": null,\n","    \"suppress_tokens\": null,\n","    \"task_specific_params\": null,\n","    \"temperature\": 1.0,\n","    \"tf_legacy_loss\": false,\n","    \"tie_encoder_decoder\": false,\n","    \"tie_word_embeddings\": true,\n","    \"tokenizer_class\": null,\n","    \"top_k\": 50,\n","    \"top_p\": 1.0,\n","    \"torch_dtype\": null,\n","    \"torchscript\": false,\n","    \"transformers_version\": \"4.28.1\",\n","    \"type_vocab_size\": 2,\n","    \"typical_p\": 1.0,\n","    \"use_bfloat16\": false,\n","    \"use_cache\": true,\n","    \"vocab_size\": 30522\n","  },\n","  \"encoder\": {\n","    \"_name_or_path\": \"\",\n","    \"add_cross_attention\": false,\n","    \"architectures\": null,\n","    \"attention_probs_dropout_prob\": 0.0,\n","    \"bad_words_ids\": null,\n","    \"begin_suppress_tokens\": null,\n","    \"bos_token_id\": null,\n","    \"chunk_size_feed_forward\": 0,\n","    \"cross_attention_hidden_size\": null,\n","    \"decoder_start_token_id\": null,\n","    \"diversity_penalty\": 0.0,\n","    \"do_sample\": false,\n","    \"early_stopping\": false,\n","    \"encoder_no_repeat_ngram_size\": 0,\n","    \"encoder_stride\": 16,\n","    \"eos_token_id\": null,\n","    \"exponential_decay_length_penalty\": null,\n","    \"finetuning_task\": null,\n","    \"forced_bos_token_id\": null,\n","    \"forced_eos_token_id\": null,\n","    \"hidden_act\": \"gelu\",\n","    \"hidden_dropout_prob\": 0.0,\n","    \"hidden_size\": 768,\n","    \"id2label\": {\n","      \"0\": \"LABEL_0\",\n","      \"1\": \"LABEL_1\"\n","    },\n","    \"image_size\": 224,\n","    \"initializer_range\": 0.02,\n","    \"intermediate_size\": 3072,\n","    \"is_decoder\": false,\n","    \"is_encoder_decoder\": false,\n","    \"label2id\": {\n","      \"LABEL_0\": 0,\n","      \"LABEL_1\": 1\n","    },\n","    \"layer_norm_eps\": 1e-12,\n","    \"length_penalty\": 1.0,\n","    \"max_length\": 20,\n","    \"min_length\": 0,\n","    \"model_type\": \"vit\",\n","    \"no_repeat_ngram_size\": 0,\n","    \"num_attention_heads\": 12,\n","    \"num_beam_groups\": 1,\n","    \"num_beams\": 1,\n","    \"num_channels\": 3,\n","    \"num_hidden_layers\": 12,\n","    \"num_return_sequences\": 1,\n","    \"output_attentions\": false,\n","    \"output_hidden_states\": false,\n","    \"output_scores\": false,\n","    \"pad_token_id\": null,\n","    \"patch_size\": 16,\n","    \"prefix\": null,\n","    \"problem_type\": null,\n","    \"pruned_heads\": {},\n","    \"qkv_bias\": true,\n","    \"remove_invalid_values\": false,\n","    \"repetition_penalty\": 1.0,\n","    \"return_dict\": true,\n","    \"return_dict_in_generate\": false,\n","    \"sep_token_id\": null,\n","    \"suppress_tokens\": null,\n","    \"task_specific_params\": null,\n","    \"temperature\": 1.0,\n","    \"tf_legacy_loss\": false,\n","    \"tie_encoder_decoder\": false,\n","    \"tie_word_embeddings\": true,\n","    \"tokenizer_class\": null,\n","    \"top_k\": 50,\n","    \"top_p\": 1.0,\n","    \"torch_dtype\": null,\n","    \"torchscript\": false,\n","    \"transformers_version\": \"4.28.1\",\n","    \"typical_p\": 1.0,\n","    \"use_bfloat16\": false\n","  },\n","  \"is_encoder_decoder\": true,\n","  \"model_type\": \"vision-encoder-decoder\",\n","  \"transformers_version\": null\n","}"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import VisionEncoderDecoderConfig\n","\n","config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder) #Create config Vit-Bert using the configuration of ViT and Bert\n","config"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:22:18.848806Z","iopub.status.busy":"2023-05-16T06:22:18.848081Z","iopub.status.idle":"2023-05-16T06:22:24.291329Z","shell.execute_reply":"2023-05-16T06:22:24.290224Z","shell.execute_reply.started":"2023-05-16T06:22:18.848755Z"},"trusted":true},"outputs":[],"source":["#For Tensorflow\n","#from transformers import TFVisionEncoderDecoderModel #Load tensorflow based VisionEncoderDecoderModel. Model to generate text from image\n","#model = TFVisionEncoderDecoderModel(config = config) #Build the model using the config\n","\n","#For pytorch\n","from transformers import VisionEncoderDecoderModel\n","model = VisionEncoderDecoderModel(config = config)"]},{"cell_type":"markdown","metadata":{},"source":["### Update the model config with the token ###"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:22:24.293247Z","iopub.status.busy":"2023-05-16T06:22:24.292871Z","iopub.status.idle":"2023-05-16T06:22:24.304437Z","shell.execute_reply":"2023-05-16T06:22:24.301105Z","shell.execute_reply.started":"2023-05-16T06:22:24.293195Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('[PAD]', '[SEP]')"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["(tokenizer.pad_token , tokenizer.eos_token)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:22:24.306523Z","iopub.status.busy":"2023-05-16T06:22:24.305843Z","iopub.status.idle":"2023-05-16T06:22:24.316846Z","shell.execute_reply":"2023-05-16T06:22:24.315111Z","shell.execute_reply.started":"2023-05-16T06:22:24.306485Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(0, 102)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["(tokenizer.pad_token_id , tokenizer.eos_token_id)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:22:24.318730Z","iopub.status.busy":"2023-05-16T06:22:24.318230Z","iopub.status.idle":"2023-05-16T06:22:24.325404Z","shell.execute_reply":"2023-05-16T06:22:24.324283Z","shell.execute_reply.started":"2023-05-16T06:22:24.318697Z"},"trusted":true},"outputs":[],"source":["model.config.eos_token_id = tokenizer.eos_token_id\n","model.config.decoder_start_token_id = tokenizer.bos_token_id\n","model.config.pad_token_id = tokenizer.pad_token_id\n","\n","#model.config.decoder.eos_token_id = tokenizer.eos_token_id\n","#model.config.decoder.decoder_start_token_id = tokenizer.bos_token_id\n","#model.config.decoder.pad_token_id = tokenizer.pad_token_id\n","\n","#model.config.encoder.eos_token_id = tokenizer.eos_token_id\n","#model.config.encoder.decoder_start_token_id = tokenizer.bos_token_id"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:22:24.327586Z","iopub.status.busy":"2023-05-16T06:22:24.327255Z","iopub.status.idle":"2023-05-16T06:22:24.334679Z","shell.execute_reply":"2023-05-16T06:22:24.333666Z","shell.execute_reply.started":"2023-05-16T06:22:24.327556Z"},"trusted":true},"outputs":[],"source":["#https://discuss.huggingface.co/t/error-training-vision-encoder-decoder-for-image-captioning/12090/6\n","\n","#model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"markdown","metadata":{},"source":["# Load data #\n","\n","Load the data from parquet file into dataset object and define class to apply pre-process on the data's.\n","\n","HuggingFace dataset: https://huggingface.co/docs/datasets/loading#parquet"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from datasets import load_dataset\n","import os\n","\n","\n","#Load from parquet file into huggingface dataset\n","base_path =  os.path.join(PROJECT_ROOT, 'data', 'processed')\n","train_data = 'train_data_processed.parquet'\n","valid_data = 'validate_data_processed.parquet'\n","\n","data_files = {\"train\": os.path.join(base_path, train_data), 'valid': os.path.join(base_path, valid_data)}\n","db_set = load_dataset(\"parquet\", data_files = data_files)\n","\n","db_set = db_set.remove_columns(['id', 'title', 'color', 'clean_title', 'clean_color', '__index_level_0__'])\n","\n","db_set = db_set.map(lambda dbrow: { \"image_path\": os.path.join(PROJECT_ROOT, 'images', dbrow[\"image_name\"]) }, remove_columns = [\"image_name\"])\n","\n","db_set\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:22:27.457957Z","iopub.status.busy":"2023-05-16T06:22:27.457137Z","iopub.status.idle":"2023-05-16T06:22:27.465627Z","shell.execute_reply":"2023-05-16T06:22:27.464436Z","shell.execute_reply.started":"2023-05-16T06:22:27.457918Z"},"trusted":true},"outputs":[{"data":{"text/plain":["18025"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["db_set['train'].num_rows"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:22:27.467841Z","iopub.status.busy":"2023-05-16T06:22:27.467428Z","iopub.status.idle":"2023-05-16T06:22:27.481748Z","shell.execute_reply":"2023-05-16T06:22:27.480503Z","shell.execute_reply.started":"2023-05-16T06:22:27.467804Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'caption': 'tomato rosehip fit flare dress',\n"," 'image_path': '/kaggle/input/fashion-image-caption-using-image-files/images/db2a1e93-3a6c-4c05-bf18-00ee6be05bd3.jpeg'}"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["db_set['train'][0]"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:22:27.484000Z","iopub.status.busy":"2023-05-16T06:22:27.483610Z","iopub.status.idle":"2023-05-16T06:22:27.521533Z","shell.execute_reply":"2023-05-16T06:22:27.520458Z","shell.execute_reply.started":"2023-05-16T06:22:27.483965Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['/kaggle/input/fashion-image-caption-using-image-files/images/db2a1e93-3a6c-4c05-bf18-00ee6be05bd3.jpeg',\n"," '/kaggle/input/fashion-image-caption-using-image-files/images/285af35b-a2b9-40d0-a153-2852a6106ba9.jpeg',\n"," '/kaggle/input/fashion-image-caption-using-image-files/images/fe4ce0e1-8306-4d61-ba7d-8665a9e9cbb9.jpeg',\n"," '/kaggle/input/fashion-image-caption-using-image-files/images/4879469f-93fb-4e01-b5e6-b03b52a49ab7.jpeg',\n"," '/kaggle/input/fashion-image-caption-using-image-files/images/b9912539-6d3b-4c73-bc82-92e3688255f1.jpeg']"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["#look at top 5 records in the train dataset\n","db_set['train'][\"image_path\"][:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Apply the pre-process on image and text on the dataset\n","#import tensorflow as tf\n","from torch.utils.data import Dataset\n","from PIL import Image\n","\n","class captiondatset(Dataset):\n","\n","    def __init__(self, datasets, length):\n","        self.datasets = datasets\n","        self.length = length\n","\n","    def __len__(self):\n","        return self.length\n","\n","    def __getitem__(self, idx):\n","        \n","        image = Image.open(self.datasets['image_path'][idx])\n","        image_features = image_processor(image, return_tensors = \"pt\").pixel_values #tf\n","\n","        labels = tokenizer(self.datasets[\"caption\"][idx], \n","                           return_tensors = \"pt\", #tf\n","                           max_length = 15,\n","                           padding = 'max_length',\n","                           return_token_type_ids = True,\n","                           truncation = True).input_ids\n","\n","        #return {'pixel_values': tf.squeeze(image_features), 'labels': tf.squeeze(labels)}\n","        return {'pixel_values': image_features.squeeze(0), 'labels': labels.squeeze(0)}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["datsets_train = captiondatset(db_set['train'], db_set['train'].num_rows)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:22:27.542877Z","iopub.status.busy":"2023-05-16T06:22:27.542358Z","iopub.status.idle":"2023-05-16T06:22:27.884205Z","shell.execute_reply":"2023-05-16T06:22:27.883155Z","shell.execute_reply.started":"2023-05-16T06:22:27.542769Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'pixel_values': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n","          [1., 1., 1.,  ..., 1., 1., 1.],\n","          [1., 1., 1.,  ..., 1., 1., 1.],\n","          ...,\n","          [1., 1., 1.,  ..., 1., 1., 1.],\n","          [1., 1., 1.,  ..., 1., 1., 1.],\n","          [1., 1., 1.,  ..., 1., 1., 1.]],\n"," \n","         [[1., 1., 1.,  ..., 1., 1., 1.],\n","          [1., 1., 1.,  ..., 1., 1., 1.],\n","          [1., 1., 1.,  ..., 1., 1., 1.],\n","          ...,\n","          [1., 1., 1.,  ..., 1., 1., 1.],\n","          [1., 1., 1.,  ..., 1., 1., 1.],\n","          [1., 1., 1.,  ..., 1., 1., 1.]],\n"," \n","         [[1., 1., 1.,  ..., 1., 1., 1.],\n","          [1., 1., 1.,  ..., 1., 1., 1.],\n","          [1., 1., 1.,  ..., 1., 1., 1.],\n","          ...,\n","          [1., 1., 1.,  ..., 1., 1., 1.],\n","          [1., 1., 1.,  ..., 1., 1., 1.],\n","          [1., 1., 1.,  ..., 1., 1., 1.]]]),\n"," 'labels': tensor([  101, 20856,  3123,  5605,  4906, 17748,  4377,   102,     0,     0,\n","             0,     0,     0,     0,     0])}"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["#Check the transformed output at row idx 0\n","datsets_train.__getitem__(0)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:22:27.886764Z","iopub.status.busy":"2023-05-16T06:22:27.885656Z","iopub.status.idle":"2023-05-16T06:22:28.166668Z","shell.execute_reply":"2023-05-16T06:22:28.165678Z","shell.execute_reply.started":"2023-05-16T06:22:27.886726Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([  101,  4462,  2995,  2317,  4438,  7540,  2006, 13583,  2121,   102,\n","            0,     0,     0,     0,     0])"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["datsets_train.__getitem__(16)['labels']"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:22:28.168994Z","iopub.status.busy":"2023-05-16T06:22:28.168133Z","iopub.status.idle":"2023-05-16T06:22:28.173741Z","shell.execute_reply":"2023-05-16T06:22:28.172834Z","shell.execute_reply.started":"2023-05-16T06:22:28.168960Z"},"trusted":true},"outputs":[],"source":["dataset_val = captiondatset(db_set['valid'], db_set['valid'].num_rows) "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import gc\n","\n","del [db_set]\n","gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluate #\n","\n","Define evaluate metric for the model."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from rouge_score import rouge_scorer, scoring\n","\n","rouge_types = [\"rouge1\", \"rouge2\", \"rougeL\"]\n","use_stemmer = False\n","\n","rouge_score_obj = rouge_scorer.RougeScorer(rouge_types = rouge_types, use_stemmer = use_stemmer)\n","aggregator = scoring.BootstrapAggregator()\n","\n","#def compute_metrics(pred):\n","def compute_metrics(pred_ids, labels_ids):\n","\n","    #labels_ids = pred.label_ids\n","    #pred_ids = pred.predictions\n","\n","    # all unnecessary tokens are removed\n","    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens = True)\n","\n","    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n","    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens = True)\n","\n","    for ref, pred in zip(label_str, pred_str):\n","\n","        output_score = rouge_score_obj.score(prediction = pred, target = ref)\n","        aggregator.add_scores(output_score)\n","\n","    result = aggregator.aggregate()\n","    \n","    return {\n","        \"rouge1_fmeasure\": round(result['rouge1'].mid.fmeasure, 2),\n","        \"rouge2_fmeasure\": round(result['rouge2'].mid.fmeasure, 2),\n","        \"rougeL_fmeasure\": round(result['rougeL'].mid.fmeasure, 2),\n","    }\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training #"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["os.environ['WANDB_DISABLED'] = 'True' #When training in Kaggel it was getting connected to WANDB.io and was asking for API"]},{"cell_type":"markdown","metadata":{},"source":["## Training Arguments ##\n","\n","Help Refer: https://huggingface.co/transformers/v3.5.1/main_classes/trainer.html#transformers.TrainingArguments"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Where model checkpoints will be saved.\n","output_dir = './data/vit_bert/'\n","\n","\n","# evaluating the trained model on the evaluation set every eval_steps training steps \n","# evaluation_strategy (default \"no\"):\n","# Possible values are:\n","# \"no\": No evaluation is done during training.\n","# \"steps\": Evaluation is done (and logged) every eval_steps paramater.\n","# \"epoch\": Evaluation is done at the end of each epoch.   \n","evaluation_strategy = 'no'\n","#eval_steps = 100\n","\n","\n","# logging_strategy (default: \"steps\"): The logging strategy to adopt during. These writen training logs will be used by TensorBoard visualized.\n","# training (used to log training loss for example). Possible values are:\n","# \"no\": No logging is done during training.\n","# \"epoch\": Logging is done at the end of each epoch.\n","# \"steps\": Logging is done every logging_steps.\n","logging_strategy = \"steps\"\n","# logging_steps (default 500): Number of update steps between two logs if\n","# logging_strategy=\"steps\".\n","logging_steps = 50\n","\n","\n","# Save the trained model.\n","# The checkpoint save strategy to adopt during training. Possible values are:\n","# \"no\": No save is done during training.\n","# \"epoch\": Save is done at the end of each epoch.\n","# \"steps\": Save is done every save_steps (default 500).\n","save_strategy = \"epoch\"\n","# save_steps (default: 500): Number of updates steps before two checkpoint\n","# saves if save_strategy=\"steps\".\n","#save_steps = 200\n","\n","\n","learning_rate = 2e-4\n","\n","\n","# per_device_train_batch_size: The batch size per GPU/TPU core/CPU for training.\n","per_device_train_batch_size = 8 #4\n","gradient_accumulation_steps = 8 #When use gradient_accumulation_steps with per_device_train_batch_size it has same effective batch size 64 when per_batch_size = 4 and grad_steps = 16\n","\n","# per_device_eval_batch_size: The batch size per GPU/TPU core/CPU for evaluation.\n","per_device_eval_batch_size = 64 #1\n","\n","\n","fp16 = True\n","\n","\n","# Total number of training epochs to perform\n","num_train_epochs = 3\n","    \n","\n","# load_best_model_at_end (default False): Whether or not to load the best model\n","# found during training at the end of training.\n","load_best_model_at_end = False\n","\n","\n","# metric_for_best_model:\n","# Use in conjunction with load_best_model_at_end to specify the metric to use\n","# to compare two different models. Must be the name of a metric returned by\n","# the evaluation with or without the prefix \"eval_\".\n","metric_for_best_model = compute_metrics\n"," \n","# report_to:\n","# The list of integrations to report the results and logs to. Supported\n","# platforms are \"azure_ml\", \"comet_ml\", \"mlflow\", \"tensorboard\" and \"wandb\".\n","# Use \"all\" to report to all integrations installed, \"none\" for no integrations.\n","report_to = None #\"tensorboard\". \n","\n","\n","from transformers import TrainingArguments\n","\n","training_args = TrainingArguments(\n","                                output_dir = output_dir,         \n","                                                                \n","                                learning_rate = learning_rate,\n","\n","                                num_train_epochs = num_train_epochs,                  \n","\n","                                per_device_train_batch_size = per_device_train_batch_size,       \n","                                gradient_accumulation_steps = gradient_accumulation_steps,\n","                                per_device_eval_batch_size = per_device_eval_batch_size,        \n","                                \n","                                fp16 = fp16,\n","\n","                                evaluation_strategy = evaluation_strategy,\n","                                #eval_steps = eval_steps,\n","\n","                                logging_strategy  = logging_strategy,\n","                                #logging_dir = './data/logs',\n","                                logging_steps  = logging_steps, \n","\n","                                save_strategy  = save_strategy,\n","\n","                                load_best_model_at_end = load_best_model_at_end,    \n","\n","                                report_to = report_to,                           \n","                                \n",")"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:22:29.494201Z","iopub.status.busy":"2023-05-16T06:22:29.493559Z","iopub.status.idle":"2023-05-16T06:22:29.498976Z","shell.execute_reply":"2023-05-16T06:22:29.497810Z","shell.execute_reply.started":"2023-05-16T06:22:29.494166Z"},"trusted":true},"outputs":[],"source":["# Start TensorBoard before training to monitor it in progress\n","\n","#%load_ext tensorboard\n","#%tensorboard --logdir output_dir\n"]},{"cell_type":"markdown","metadata":{},"source":["## Train ##\n","\n","Help refer: https://huggingface.co/docs/transformers/training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#For tensorflow\n","\n","#import tensorflow as tf\n","#devices = tf.config.experimental.list_physical_devices('GPU')\n","\n","#For Pytorch\n","import torch\n","\n","devices = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","model = model.to(devices)\n","model.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import default_data_collator\n","from transformers import Trainer\n","\n","#For train tensorflow: using compile and fit methods of tf.\n","\n","#Train with PyTorch use Trainer Class Object\n","trainer = Trainer(\n","                model = model,                         # the instantiated Transformers model to be trained\n","                args = training_args,                  # training arguments, defined above\n","                train_dataset = datsets_train,         # training dataset\n","                #eval_dataset = dataset_val,             # evaluation dataset\n","                #compute_metrics = metric_for_best_model,\n","                data_collator = default_data_collator,\n",")"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:22:35.077906Z","iopub.status.busy":"2023-05-16T06:22:35.077261Z","iopub.status.idle":"2023-05-16T10:30:29.019413Z","shell.execute_reply":"2023-05-16T10:30:29.014079Z","shell.execute_reply.started":"2023-05-16T06:22:35.077870Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["Tracking run with wandb version 0.15.0"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='420' max='420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [420/420 4:06:14, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>50</td>\n","      <td>6.549600</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>4.202300</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>4.206900</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>4.694600</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>4.684000</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>4.652900</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>4.612600</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>4.627100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/plain":["TrainOutput(global_step=420, training_loss=4.768845930553618, metrics={'train_runtime': 14873.9003, 'train_samples_per_second': 3.636, 'train_steps_per_second': 0.028, 'total_flos': 9.729436761304179e+18, 'train_loss': 4.768845930553618, 'epoch': 2.98})"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-05-17T09:24:30.128313Z","iopub.status.busy":"2023-05-17T09:24:30.127760Z","iopub.status.idle":"2023-05-17T13:50:04.514506Z","shell.execute_reply":"2023-05-17T13:50:04.504383Z","shell.execute_reply.started":"2023-05-17T09:24:30.128269Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='420' max='420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [420/420 4:24:42, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>50</td>\n","      <td>4.670700</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>4.648900</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>4.583200</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>4.637900</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>4.624000</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>4.568900</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>4.505900</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>4.513700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]}],"source":["if pre_trained_model_exists == True: #If pre trained model checkpoint exists continue train from that location\n","    trainer.train() #Continue training"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-05-17T13:51:02.668681Z","iopub.status.busy":"2023-05-17T13:51:02.668281Z","iopub.status.idle":"2023-05-17T13:51:05.980321Z","shell.execute_reply":"2023-05-17T13:51:05.978852Z","shell.execute_reply.started":"2023-05-17T13:51:02.668633Z"},"trusted":true},"outputs":[],"source":["trainer.save_model(os.path.join(PROJECT_ROOT, 'data', 'Vit_bert_based_checkpoint'))"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-05-17T13:51:05.985514Z","iopub.status.busy":"2023-05-17T13:51:05.985136Z","iopub.status.idle":"2023-05-17T13:52:00.416054Z","shell.execute_reply":"2023-05-17T13:52:00.414890Z","shell.execute_reply.started":"2023-05-17T13:51:05.985483Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","Would zip the file for easier download\n","import zipfile\n","import os\n","\n","def zipdir(path, ziph):\n","    # ziph is zipfile handle\n","    for root, dirs, files in os.walk(path):\n","        for file in files:\n","            ziph.write(os.path.join(root, file))\n","            \n","zip_filename = os.path.join(PROJECT_ROOT, 'data','Vit_bert_based_checkpoint.zip')  \n","zipf = zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED)\n","zipdir(os.path.join(PROJECT_ROOT, 'data', 'Vit_bert_based_checkpoint'), zipf)\n","zipf.close()  \n","\n","from IPython.display import FileLink\n","FileLink(r'Vit_bert_based_checkpoint.zip')\n","\n","\n","\"\"\""]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T10:41:16.468917Z","iopub.status.busy":"2023-05-16T10:41:16.468526Z","iopub.status.idle":"2023-05-16T10:41:16.477886Z","shell.execute_reply":"2023-05-16T10:41:16.476634Z","shell.execute_reply.started":"2023-05-16T10:41:16.468883Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","import shutil\n","shutil.rmtree(./Vit_bert_based_checkpoint.zip\")\n","\n","zip_filename = os.path.join(os.getcwd(),'data.zip')  \n","zipf = zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED)\n","zipdir(os.path.join(os.getcwd(), 'data'), zipf)\n","zipf.close() \n","\n","shutil.rmtree(\"data\")\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#evaluate after 2nd run for train data\n","\n","generated_ids = model.generate(datsets_train['pixel_values'])\n","#train_eval_result = trainer.evaluate(datsets_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["metric_result = compute_metrics(generated_ids, datsets_train['labels'])\n","metric_result"]}],"metadata":{"interpreter":{"hash":"7a8019b6faff98163eb884b931cd42fce4cc159f4b6cbdadf8184d77ab18833d"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
